{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Target Encoding",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "wlMOSchK2noH",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Target Encoding\n",
        "\n",
        "TODO: intro about ways to encode categorical cols: label, one-hot, and entity embbeddings (if you were using a neural net or gradient descent-based method, but can't do that if you want to use say xgboost).  Problem w/ label encoding is doesn't work at all w/ non-ordinal features w/ a high # of categories (or at all at all w/ a linear model).  Problem w/ one-hot is that it can create a crazy number of columns.  \n",
        "\n",
        "TODO: One solution is to use target encoding, where you replace each category with the mean target value for samples having that category.  However, blindly using target encoding can allow data leakage, leading to very poor cross-validated or validation performance!  To fix that problem, have to do cross-fold target encoding.\n",
        "\n",
        "TODO: but even with cross-fold target encoding, there are situations where you'd be better off with one-hot or other encoding methods: one-hot is usually better in situations with few categories, and one-hot is definitely better when there are important interaction effects.\n",
        "\n",
        "In this post we'll evaluate different encoding schemes, build a cross-fold target encoder to mitigate the drawbacks of the naive target encoder, and determine how the performance of predictive models change based on the type of category encoding used, the number of categories in the dataset, and the presence of interaction effects. \n",
        "\n",
        "TODO: outline\n",
        "\n",
        "First let's import the packages we'll be using."
      ]
    },
    {
      "metadata": {
        "id": "fXS8EKjT2o7n",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn.metrics import mean_absolute_error, make_scorer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.model_selection import KFold \n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.linear_model import BayesianRidge\n",
        "from sklearn.linear_model import Lasso\n",
        "from xgboost import XGBRegressor\n",
        "\n",
        "#Regressor = BayesianRidge()\n",
        "Regressor = Lasso(alpha=0.5)\n",
        "\n",
        "import warnings\n",
        "from sklearn.exceptions import DataConversionWarning\n",
        "warnings.filterwarnings(action='ignore', category=DataConversionWarning)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8LQUOJ8D2q6f",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Data\n",
        "\n",
        "To evaluate the effectiveness of different encoding algorithms, we'll want to be able to generate data with different numbers of samples, features, and categories.  Let's make a function to generate categorical datasets, which allows us to set these different aspects of the data."
      ]
    },
    {
      "metadata": {
        "id": "KUvdnIilBDaN",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "TODO: think there's a bug somewhere in the data-generating func - w/ noise=1, even w/ perfect inference should only be able to get down to MAE of ~0.8, and OHE gets down to like 0.23...  And then cross-fold TE is worse than chance..."
      ]
    },
    {
      "metadata": {
        "id": "31A0dwV-Lohj",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def make_categorical_regression(n_samples=100,\n",
        "                                n_features=10,\n",
        "                                n_informative=10,\n",
        "                                n_categories=10,\n",
        "                                noise=1.0,\n",
        "                                n_cont_features=10,\n",
        "                                cont_weight=0.1):\n",
        "    \"\"\"Generate a regression problem with only categorical features.\n",
        "  \n",
        "    Parameters\n",
        "    ----------\n",
        "    n_samples : int\n",
        "        Number of samples to generate\n",
        "    n_features : int\n",
        "        Number of categorical features to generate\n",
        "    n_informative : int\n",
        "        Number of features which carry information about the target\n",
        "    n_categories : int or list or ndarray\n",
        "        Number of categories per feature.\n",
        "    noise : float\n",
        "        Noise to add to target\n",
        "    n_cont_features : int\n",
        "        Number of continuous (non-categorical) features.\n",
        "    cont_weight : float\n",
        "        Weight of the continuous variables' effect.\n",
        "        \n",
        "    Returns\n",
        "    -------\n",
        "    X : pandas DataFrame of shape (n_samples, n_features)\n",
        "        Categorical features.\n",
        "    y : pandas Series of shape (n_samples,)\n",
        "        Target variable.\n",
        "    \"\"\"\n",
        "\n",
        "    # Check inputs\n",
        "    if not isinstance(n_samples, int):\n",
        "        raise TypeError('n_samples must be an int')\n",
        "    if n_samples < 1:\n",
        "        raise ValueError('n_samples must be one or greater')\n",
        "    if not isinstance(n_features, int):\n",
        "        raise TypeError('n_features must be an int')\n",
        "    if n_features < 1:\n",
        "        raise ValueError('n_features must be one or greater')\n",
        "    if not isinstance(n_informative, int):\n",
        "        raise TypeError('n_informative must be an int')\n",
        "    if n_informative < 1:\n",
        "        raise ValueError('n_informative must be one or greater')\n",
        "    if not isinstance(n_categories, int):\n",
        "        raise TypeError('n_categories must be an int')\n",
        "    if n_categories < 1:\n",
        "        raise ValueError('n_categories must be one or greater')\n",
        "    if not isinstance(noise, float):\n",
        "        raise TypeError('noise must be a float')\n",
        "    if noise < 0:\n",
        "        raise ValueError('noise must be positive')\n",
        "    if not isinstance(n_cont_features, int):\n",
        "        raise TypeError('n_cont_features must be an int')\n",
        "    if n_cont_features < 0:\n",
        "        raise ValueError('n_cont_features must be zero or greater')\n",
        "    if not isinstance(cont_weight, float):\n",
        "        raise TypeError('cont_weight must be a float')\n",
        "    if cont_weight < 0:\n",
        "        raise ValueError('cont_weight must be positive')\n",
        "        \n",
        "    # Generate random categorical data\n",
        "    categories = np.random.randint(n_categories,\n",
        "                                   size=(n_samples, n_features))\n",
        "    \n",
        "    # Generate random values for each category\n",
        "    cat_vals = np.random.randn(n_categories, n_features)\n",
        "    \n",
        "    # Set non-informative columns' effect to 0\n",
        "    cat_vals[:,:(n_features-n_informative)] = 0\n",
        "    \n",
        "    # Compute target variable from those categories and their values\n",
        "    y = np.zeros(n_samples)\n",
        "    for iC in range(n_features):\n",
        "      y += cat_vals[categories[:,iC], iC]\n",
        "    \n",
        "    # Add noise\n",
        "    y += noise*np.random.randn(n_samples)\n",
        "    \n",
        "    # Generate dataframe from categories\n",
        "    cat_strs = [''.join([chr(ord(c)+49) for c in str(n)]) \n",
        "                for n in range(n_categories)]\n",
        "    X = pd.DataFrame()\n",
        "    for iC in range(n_features):\n",
        "        col_str = 'categorical_'+str(iC)\n",
        "        X[col_str] = [cat_strs[i] for i in categories[:,iC]]\n",
        "        \n",
        "    # Add continuous features\n",
        "    for iC in range(n_cont_features):\n",
        "        col_str = 'continuous_'+str(iC)\n",
        "        X[col_str] = cont_weight*np.random.randn(n_samples)\n",
        "        y += np.random.randn()*X[col_str]\n",
        "                    \n",
        "    # Generate series from target\n",
        "    y = pd.Series(data=y, index=X.index)\n",
        "    \n",
        "    # Return features and target\n",
        "    return X, y"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "AwugyiOT3xgn",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Now, we can easily generate data to test our encoders on:"
      ]
    },
    {
      "metadata": {
        "id": "RnHHmJlIw-AK",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Generate categorical data and target\n",
        "X, y = make_categorical_regression(n_samples=2000,\n",
        "                                   n_features=10,\n",
        "                                   n_categories=100,\n",
        "                                   n_cont_features=0)\n",
        "\n",
        "# Split into test and training data\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "XP8jiuvu4InP",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The features are all categorical (stored as \"objects\"):"
      ]
    },
    {
      "metadata": {
        "id": "iBiFTq5S4Mtn",
        "colab_type": "code",
        "outputId": "d63be671-4eb3-4ad9-f2f2-5a36a037c2ed",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 379
        }
      },
      "cell_type": "code",
      "source": [
        "X_train.sample(10)"
      ],
      "execution_count": 795,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>categorical_0</th>\n",
              "      <th>categorical_1</th>\n",
              "      <th>categorical_2</th>\n",
              "      <th>categorical_3</th>\n",
              "      <th>categorical_4</th>\n",
              "      <th>categorical_5</th>\n",
              "      <th>categorical_6</th>\n",
              "      <th>categorical_7</th>\n",
              "      <th>categorical_8</th>\n",
              "      <th>categorical_9</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>607</th>\n",
              "      <td>bg</td>\n",
              "      <td>hg</td>\n",
              "      <td>fe</td>\n",
              "      <td>gb</td>\n",
              "      <td>jc</td>\n",
              "      <td>a</td>\n",
              "      <td>fj</td>\n",
              "      <td>fb</td>\n",
              "      <td>cc</td>\n",
              "      <td>eb</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1284</th>\n",
              "      <td>dc</td>\n",
              "      <td>i</td>\n",
              "      <td>fb</td>\n",
              "      <td>eg</td>\n",
              "      <td>cg</td>\n",
              "      <td>c</td>\n",
              "      <td>jb</td>\n",
              "      <td>ef</td>\n",
              "      <td>gc</td>\n",
              "      <td>dd</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>364</th>\n",
              "      <td>bi</td>\n",
              "      <td>jg</td>\n",
              "      <td>hf</td>\n",
              "      <td>if</td>\n",
              "      <td>eb</td>\n",
              "      <td>ii</td>\n",
              "      <td>cf</td>\n",
              "      <td>if</td>\n",
              "      <td>fa</td>\n",
              "      <td>fi</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>301</th>\n",
              "      <td>h</td>\n",
              "      <td>fb</td>\n",
              "      <td>ec</td>\n",
              "      <td>eg</td>\n",
              "      <td>hg</td>\n",
              "      <td>hb</td>\n",
              "      <td>fe</td>\n",
              "      <td>ba</td>\n",
              "      <td>d</td>\n",
              "      <td>bi</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>104</th>\n",
              "      <td>id</td>\n",
              "      <td>hh</td>\n",
              "      <td>ij</td>\n",
              "      <td>ch</td>\n",
              "      <td>fd</td>\n",
              "      <td>hb</td>\n",
              "      <td>f</td>\n",
              "      <td>ij</td>\n",
              "      <td>eh</td>\n",
              "      <td>ch</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1852</th>\n",
              "      <td>ji</td>\n",
              "      <td>fb</td>\n",
              "      <td>dc</td>\n",
              "      <td>he</td>\n",
              "      <td>di</td>\n",
              "      <td>gi</td>\n",
              "      <td>gb</td>\n",
              "      <td>he</td>\n",
              "      <td>ei</td>\n",
              "      <td>if</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1407</th>\n",
              "      <td>bf</td>\n",
              "      <td>cg</td>\n",
              "      <td>fg</td>\n",
              "      <td>di</td>\n",
              "      <td>hg</td>\n",
              "      <td>ii</td>\n",
              "      <td>bi</td>\n",
              "      <td>j</td>\n",
              "      <td>cc</td>\n",
              "      <td>bd</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>886</th>\n",
              "      <td>cg</td>\n",
              "      <td>bd</td>\n",
              "      <td>fa</td>\n",
              "      <td>gf</td>\n",
              "      <td>fe</td>\n",
              "      <td>ij</td>\n",
              "      <td>ee</td>\n",
              "      <td>b</td>\n",
              "      <td>jj</td>\n",
              "      <td>fe</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>947</th>\n",
              "      <td>bh</td>\n",
              "      <td>hb</td>\n",
              "      <td>ga</td>\n",
              "      <td>ha</td>\n",
              "      <td>gj</td>\n",
              "      <td>ge</td>\n",
              "      <td>gi</td>\n",
              "      <td>fb</td>\n",
              "      <td>dc</td>\n",
              "      <td>hi</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1848</th>\n",
              "      <td>ea</td>\n",
              "      <td>hc</td>\n",
              "      <td>ce</td>\n",
              "      <td>he</td>\n",
              "      <td>hc</td>\n",
              "      <td>jf</td>\n",
              "      <td>ge</td>\n",
              "      <td>bf</td>\n",
              "      <td>ji</td>\n",
              "      <td>gh</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     categorical_0 categorical_1 categorical_2 categorical_3 categorical_4  \\\n",
              "607             bg            hg            fe            gb            jc   \n",
              "1284            dc             i            fb            eg            cg   \n",
              "364             bi            jg            hf            if            eb   \n",
              "301              h            fb            ec            eg            hg   \n",
              "104             id            hh            ij            ch            fd   \n",
              "1852            ji            fb            dc            he            di   \n",
              "1407            bf            cg            fg            di            hg   \n",
              "886             cg            bd            fa            gf            fe   \n",
              "947             bh            hb            ga            ha            gj   \n",
              "1848            ea            hc            ce            he            hc   \n",
              "\n",
              "     categorical_5 categorical_6 categorical_7 categorical_8 categorical_9  \n",
              "607              a            fj            fb            cc            eb  \n",
              "1284             c            jb            ef            gc            dd  \n",
              "364             ii            cf            if            fa            fi  \n",
              "301             hb            fe            ba             d            bi  \n",
              "104             hb             f            ij            eh            ch  \n",
              "1852            gi            gb            he            ei            if  \n",
              "1407            ii            bi             j            cc            bd  \n",
              "886             ij            ee             b            jj            fe  \n",
              "947             ge            gi            fb            dc            hi  \n",
              "1848            jf            ge            bf            ji            gh  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 795
        }
      ]
    },
    {
      "metadata": {
        "id": "yxaiV1qjnpJb",
        "colab_type": "code",
        "outputId": "9d24d3fa-7908-45ce-e118-8fb2435c92ae",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        }
      },
      "cell_type": "code",
      "source": [
        "X_train.info()"
      ],
      "execution_count": 796,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Int64Index: 1000 entries, 655 to 937\n",
            "Data columns (total 10 columns):\n",
            "categorical_0    1000 non-null object\n",
            "categorical_1    1000 non-null object\n",
            "categorical_2    1000 non-null object\n",
            "categorical_3    1000 non-null object\n",
            "categorical_4    1000 non-null object\n",
            "categorical_5    1000 non-null object\n",
            "categorical_6    1000 non-null object\n",
            "categorical_7    1000 non-null object\n",
            "categorical_8    1000 non-null object\n",
            "categorical_9    1000 non-null object\n",
            "dtypes: object(10)\n",
            "memory usage: 85.9+ KB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Ckap_xbm-ZFo",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "But the target variable is continuous:"
      ]
    },
    {
      "metadata": {
        "id": "y_82h44JyBW7",
        "colab_type": "code",
        "outputId": "d5f5444d-f5ee-413a-ff00-1ed519ad8e72",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 364
        }
      },
      "cell_type": "code",
      "source": [
        "y_train.hist(bins=20)"
      ],
      "execution_count": 797,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7fd21e9aa400>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 797
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAeQAAAFKCAYAAADMuCxnAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 576x396 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "sH3ADlvjAbo6",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Let's see which category encoding scheme best allows us to predict the target variable!"
      ]
    },
    {
      "metadata": {
        "id": "7gsbU-a4ArVl",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Baseline\n",
        "\n",
        "For comparison, how well would we do if we just predicted the mean?  We'll use the mean absolute error as our performance metric."
      ]
    },
    {
      "metadata": {
        "id": "UTQOgHegAqhh",
        "colab_type": "code",
        "outputId": "431f8f16-0462-4d7b-8601-d644e33736f4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "mean_absolute_error(y_train, \n",
        "                    np.full(y_train.shape[0], y_train.mean()))"
      ],
      "execution_count": 798,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2.650580909372541"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 798
        }
      ]
    },
    {
      "metadata": {
        "id": "tPKR7mZICTrT",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "So, we should definitely be shooting for a mean absolute error of less than 1.64!"
      ]
    },
    {
      "metadata": {
        "id": "F3spAaSlLozj",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Label Encoding\n",
        "\n",
        "TODO: prediction w/ simple label encoding, which is just replacing each unique category w/ a unique integer\n",
        "\n",
        "TODO: note that we could also use Scikit-learn's LabelEncoder"
      ]
    },
    {
      "metadata": {
        "id": "pFPw2LQZggHr",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class LabelEncoder(BaseEstimator, TransformerMixin):\n",
        "    \"\"\"Label encoder.\n",
        "    \n",
        "    Replaces categorical column(s) with integer labels for each unique\n",
        "    category in original column.\n",
        "\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, cols=None):\n",
        "        \"\"\"Label encoder.\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        cols : list of str\n",
        "            Columns to label encode.  Default is to label encode \n",
        "            all categorical columns in the DataFrame.\n",
        "        \"\"\"\n",
        "        if isinstance(cols, str):\n",
        "            self.cols = [cols]\n",
        "        else:\n",
        "            self.cols = cols\n",
        "        \n",
        "        \n",
        "    def fit(self, X, y):\n",
        "        \"\"\"Fit label encoder to X and y\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        X : pandas DataFrame, shape [n_samples, n_columns]\n",
        "            DataFrame containing columns to label encode\n",
        "        y : pandas Series, shape = [n_samples]\n",
        "            Target values.\n",
        "            \n",
        "        Returns\n",
        "        -------\n",
        "        self : encoder\n",
        "            Returns self.\n",
        "        \"\"\"\n",
        "        \n",
        "        # Encode all categorical cols by default\n",
        "        if self.cols is None:\n",
        "            self.cols = [c for c in X if str(X[c].dtype)=='object']\n",
        "\n",
        "        # Check columns are in X\n",
        "        for col in self.cols:\n",
        "            if col not in X:\n",
        "                raise ValueError('Column \\''+col+'\\' not in X')\n",
        "\n",
        "        # Create the map from objects to integers for each column\n",
        "        self.maps = dict() #dict to store map for each column\n",
        "        for col in self.cols:\n",
        "            self.maps[col] = dict(zip(\n",
        "                X[col].values, \n",
        "                X[col].astype('category').cat.codes.values\n",
        "            ))\n",
        "                        \n",
        "        # Return fit object\n",
        "        return self\n",
        "\n",
        "        \n",
        "    def transform(self, X, y=None):\n",
        "        \"\"\"Perform the label encoding transformation.\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        X : pandas DataFrame, shape [n_samples, n_columns]\n",
        "            DataFrame containing columns to label encode\n",
        "            \n",
        "        Returns\n",
        "        -------\n",
        "        pandas DataFrame\n",
        "            Input DataFrame with transformed columns\n",
        "        \"\"\"\n",
        "        Xo = X.copy()\n",
        "        for col, tmap in self.maps.items():\n",
        "          \n",
        "            # Map the column\n",
        "            Xo[col] = Xo[col].map(tmap)\n",
        "            \n",
        "            # Convert to appropriate datatype\n",
        "            max_val = max(tmap.values())\n",
        "            if Xo[col].isnull().any(): #nulls, so have to use float!\n",
        "                if max_val < 8388608:\n",
        "                    dtype = 'float32'\n",
        "                else:\n",
        "                    dtype = 'float64'\n",
        "            else:\n",
        "                if max_val < 256:\n",
        "                    dtype = 'uint8'\n",
        "                elif max_val < 65536:\n",
        "                    dtype = 'uint16'\n",
        "                elif max_val < 4294967296:\n",
        "                    dtype = 'uint32'\n",
        "                else:\n",
        "                    dtype = 'uint64'\n",
        "            Xo[col] = Xo[col].astype(dtype)\n",
        "            \n",
        "        # Return encoded dataframe\n",
        "        return Xo\n",
        "            \n",
        "            \n",
        "    def fit_transform(self, X, y=None):\n",
        "        \"\"\"Fit and transform the data via label encoding.\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        X : pandas DataFrame, shape [n_samples, n_columns]\n",
        "            DataFrame containing columns to label encode\n",
        "        y : pandas Series, shape = [n_samples]\n",
        "            Target values\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        pandas DataFrame\n",
        "            Input DataFrame with transformed columns\n",
        "        \"\"\"\n",
        "        return self.fit(X, y).transform(X, y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "nDhwfsV5DDpj",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Now we can convert the categories to integers:"
      ]
    },
    {
      "metadata": {
        "id": "Y2YJidsuC0if",
        "colab_type": "code",
        "outputId": "a5098942-eeef-4eed-8c27-cc15b393373f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 379
        }
      },
      "cell_type": "code",
      "source": [
        "# Label encode the categorical data\n",
        "le = LabelEncoder()\n",
        "X_label_encoded = le.fit_transform(X_train, y_train)\n",
        "X_label_encoded.sample(10)"
      ],
      "execution_count": 800,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>categorical_0</th>\n",
              "      <th>categorical_1</th>\n",
              "      <th>categorical_2</th>\n",
              "      <th>categorical_3</th>\n",
              "      <th>categorical_4</th>\n",
              "      <th>categorical_5</th>\n",
              "      <th>categorical_6</th>\n",
              "      <th>categorical_7</th>\n",
              "      <th>categorical_8</th>\n",
              "      <th>categorical_9</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1317</th>\n",
              "      <td>9</td>\n",
              "      <td>87</td>\n",
              "      <td>71</td>\n",
              "      <td>9</td>\n",
              "      <td>20</td>\n",
              "      <td>85</td>\n",
              "      <td>33</td>\n",
              "      <td>59</td>\n",
              "      <td>41</td>\n",
              "      <td>8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1127</th>\n",
              "      <td>55</td>\n",
              "      <td>20</td>\n",
              "      <td>54</td>\n",
              "      <td>15</td>\n",
              "      <td>20</td>\n",
              "      <td>47</td>\n",
              "      <td>21</td>\n",
              "      <td>54</td>\n",
              "      <td>65</td>\n",
              "      <td>52</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>982</th>\n",
              "      <td>82</td>\n",
              "      <td>13</td>\n",
              "      <td>16</td>\n",
              "      <td>66</td>\n",
              "      <td>48</td>\n",
              "      <td>21</td>\n",
              "      <td>88</td>\n",
              "      <td>15</td>\n",
              "      <td>64</td>\n",
              "      <td>32</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1535</th>\n",
              "      <td>51</td>\n",
              "      <td>92</td>\n",
              "      <td>92</td>\n",
              "      <td>68</td>\n",
              "      <td>25</td>\n",
              "      <td>19</td>\n",
              "      <td>50</td>\n",
              "      <td>69</td>\n",
              "      <td>76</td>\n",
              "      <td>74</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>746</th>\n",
              "      <td>70</td>\n",
              "      <td>42</td>\n",
              "      <td>69</td>\n",
              "      <td>14</td>\n",
              "      <td>19</td>\n",
              "      <td>10</td>\n",
              "      <td>61</td>\n",
              "      <td>25</td>\n",
              "      <td>96</td>\n",
              "      <td>80</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1183</th>\n",
              "      <td>17</td>\n",
              "      <td>87</td>\n",
              "      <td>12</td>\n",
              "      <td>96</td>\n",
              "      <td>11</td>\n",
              "      <td>99</td>\n",
              "      <td>4</td>\n",
              "      <td>92</td>\n",
              "      <td>86</td>\n",
              "      <td>19</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1851</th>\n",
              "      <td>18</td>\n",
              "      <td>71</td>\n",
              "      <td>64</td>\n",
              "      <td>51</td>\n",
              "      <td>44</td>\n",
              "      <td>92</td>\n",
              "      <td>30</td>\n",
              "      <td>0</td>\n",
              "      <td>54</td>\n",
              "      <td>80</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>995</th>\n",
              "      <td>96</td>\n",
              "      <td>4</td>\n",
              "      <td>5</td>\n",
              "      <td>74</td>\n",
              "      <td>4</td>\n",
              "      <td>68</td>\n",
              "      <td>13</td>\n",
              "      <td>7</td>\n",
              "      <td>36</td>\n",
              "      <td>79</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>275</th>\n",
              "      <td>14</td>\n",
              "      <td>27</td>\n",
              "      <td>68</td>\n",
              "      <td>73</td>\n",
              "      <td>74</td>\n",
              "      <td>44</td>\n",
              "      <td>96</td>\n",
              "      <td>17</td>\n",
              "      <td>20</td>\n",
              "      <td>27</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>742</th>\n",
              "      <td>16</td>\n",
              "      <td>33</td>\n",
              "      <td>4</td>\n",
              "      <td>44</td>\n",
              "      <td>79</td>\n",
              "      <td>57</td>\n",
              "      <td>99</td>\n",
              "      <td>84</td>\n",
              "      <td>91</td>\n",
              "      <td>39</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      categorical_0  categorical_1  categorical_2  categorical_3  \\\n",
              "1317              9             87             71              9   \n",
              "1127             55             20             54             15   \n",
              "982              82             13             16             66   \n",
              "1535             51             92             92             68   \n",
              "746              70             42             69             14   \n",
              "1183             17             87             12             96   \n",
              "1851             18             71             64             51   \n",
              "995              96              4              5             74   \n",
              "275              14             27             68             73   \n",
              "742              16             33              4             44   \n",
              "\n",
              "      categorical_4  categorical_5  categorical_6  categorical_7  \\\n",
              "1317             20             85             33             59   \n",
              "1127             20             47             21             54   \n",
              "982              48             21             88             15   \n",
              "1535             25             19             50             69   \n",
              "746              19             10             61             25   \n",
              "1183             11             99              4             92   \n",
              "1851             44             92             30              0   \n",
              "995               4             68             13              7   \n",
              "275              74             44             96             17   \n",
              "742              79             57             99             84   \n",
              "\n",
              "      categorical_8  categorical_9  \n",
              "1317             41              8  \n",
              "1127             65             52  \n",
              "982              64             32  \n",
              "1535             76             74  \n",
              "746              96             80  \n",
              "1183             86             19  \n",
              "1851             54             80  \n",
              "995              36             79  \n",
              "275              20             27  \n",
              "742              91             39  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 800
        }
      ]
    },
    {
      "metadata": {
        "id": "67LwHzQAW1Rl",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "But again, these integers aren't related to the categories in any meaningful way - aside from the fact that each unique integer corresponds to a unique category.\n",
        "\n",
        "TODO: then we can make a model to predict"
      ]
    },
    {
      "metadata": {
        "id": "u4Nibf2SW061",
        "colab_type": "code",
        "outputId": "eaf02d25-719c-497a-c3d0-063b9cf0a07e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "# Regression model\n",
        "model = Pipeline([\n",
        "    ('label-encoder', LabelEncoder()),\n",
        "    ('scaler', StandardScaler()),\n",
        "    ('imputer', SimpleImputer(strategy='mean')),\n",
        "    ('regressor', Regressor)\n",
        "])\n",
        "\n",
        "# Cross-validated MAE\n",
        "mae_scorer = make_scorer(mean_absolute_error)\n",
        "scores = cross_val_score(model, X_train, y_train, \n",
        "                         cv=3, scoring=mae_scorer)\n",
        "print('Cross-validated MAE:', scores.mean(), '+/-', scores.std())"
      ],
      "execution_count": 801,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cross-validated MAE: 2.6511893382762985 +/- 0.039101848009455334\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "0FU8xUdnbdX3",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "TODO: That's not much better than just predicting the mean!\n",
        "\n",
        "However, the error is no worse on the test data than the cross-validated error on the training data.\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "m9Depw3LbhbG",
        "colab_type": "code",
        "outputId": "18243be5-e3db-448b-baa8-59a2015e70f7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "# MAE on test data\n",
        "model.fit(X_train, y_train)\n",
        "y_pred = model.predict(X_test)\n",
        "test_mae = mean_absolute_error(y_test, y_pred)\n",
        "print('Validation MAE:', test_mae)"
      ],
      "execution_count": 802,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Validation MAE: 2.5913815779421268\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "g__EEWRPLuEe",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## One-hot Encoding\n",
        "\n",
        "TODO: one-hot encoding, sometimes aka \"Dummy encoding\"\n",
        "\n",
        "(note that we could also use Scikit-learn's OneHotEncoder)"
      ]
    },
    {
      "metadata": {
        "id": "xu7mOQTedGsp",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class OneHotEncoder(BaseEstimator, TransformerMixin):\n",
        "    \"\"\"One-hot encoder.\n",
        "    \n",
        "    Replaces categorical column(s) with binary columns for each unique\n",
        "    value in original column.\n",
        "\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, cols=None, reduce_df=False):\n",
        "        \"\"\"One-hot encoder.\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        cols : list of str\n",
        "            Columns to one-hot encode.  Default is to one-hot encode \n",
        "            all categorical columns in the DataFrame.\n",
        "        reduce_df : bool\n",
        "            Whether to add N-1 one-hot columns for a column with N \n",
        "            categories. E.g. for a column with categories A, B, and C:\n",
        "            When reduce_df is True, A=[1, 0], B=[0, 1], and C=[0, 0]\n",
        "            When reduce_df is False, A=[1, 0, 0], B=[0, 1, 0], and \n",
        "            C=[0, 0, 1]\n",
        "            Default = False\n",
        "        \"\"\"\n",
        "        if isinstance(cols, str):\n",
        "            self.cols = [cols]\n",
        "        else:\n",
        "            self.cols = cols\n",
        "        self.reduce_df = reduce_df\n",
        "        \n",
        "        \n",
        "    def fit(self, X, y):\n",
        "        \"\"\"Fit one-hot encoder to X and y\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        X : pandas DataFrame, shape [n_samples, n_columns]\n",
        "            DataFrame containing columns to encode\n",
        "        y : pandas Series, shape = [n_samples]\n",
        "            Target values.\n",
        "            \n",
        "        Returns\n",
        "        -------\n",
        "        self : encoder\n",
        "            Returns self.\n",
        "        \"\"\"\n",
        "        \n",
        "        # Encode all categorical cols by default\n",
        "        if self.cols is None:\n",
        "            self.cols = [c for c in X if str(X[c].dtype)=='object']\n",
        "\n",
        "        # Check columns are in X\n",
        "        for col in self.cols:\n",
        "            if col not in X:\n",
        "                raise ValueError('Column \\''+col+'\\' not in X')\n",
        "\n",
        "        # Store each unique value\n",
        "        self.maps = dict() #dict to store map for each column\n",
        "        for col in self.cols:\n",
        "            self.maps[col] = []\n",
        "            uniques = X[col].unique()\n",
        "            for unique in uniques:\n",
        "                self.maps[col].append(unique)\n",
        "            if self.reduce_df:\n",
        "                del self.maps[col][-1]\n",
        "        \n",
        "        # Return fit object\n",
        "        return self\n",
        "\n",
        "        \n",
        "    def transform(self, X, y=None):\n",
        "        \"\"\"Perform the one-hot encoding transformation.\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        X : pandas DataFrame, shape [n_samples, n_columns]\n",
        "            DataFrame containing columns to one-hot encode\n",
        "            \n",
        "        Returns\n",
        "        -------\n",
        "        pandas DataFrame\n",
        "            Input DataFrame with transformed columns\n",
        "        \"\"\"\n",
        "        Xo = X.copy()\n",
        "        for col, vals in self.maps.items():\n",
        "            for val in vals:\n",
        "                new_col = col+'_'+str(val)\n",
        "                Xo[new_col] = (Xo[col]==val).astype('uint8')\n",
        "            del Xo[col]\n",
        "        return Xo\n",
        "            \n",
        "            \n",
        "    def fit_transform(self, X, y=None):\n",
        "        \"\"\"Fit and transform the data via one-hot encoding.\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        X : pandas DataFrame, shape [n_samples, n_columns]\n",
        "            DataFrame containing columns to one-hot encode\n",
        "        y : pandas Series, shape = [n_samples]\n",
        "            Target values\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        pandas DataFrame\n",
        "            Input DataFrame with transformed columns\n",
        "        \"\"\"\n",
        "        return self.fit(X, y).transform(X, y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "AosYxSrbNBOs",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Now, instead of replacing categories with integer labels, we've create a new column for each category in each original column.  The value in a given column is 1 when the original category matches, otherwise the value is 0."
      ]
    },
    {
      "metadata": {
        "id": "Xg9TlgHDNRC-",
        "colab_type": "code",
        "outputId": "9b109caf-c8b6-4134-8cb5-1b33129ea7ca",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        }
      },
      "cell_type": "code",
      "source": [
        "# One-hot-encode the categorical data\n",
        "ohe = OneHotEncoder()\n",
        "X_one_hot = ohe.fit_transform(X_train, y_train)\n",
        "X_one_hot.sample(10)"
      ],
      "execution_count": 804,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>categorical_0_dd</th>\n",
              "      <th>categorical_0_ba</th>\n",
              "      <th>categorical_0_df</th>\n",
              "      <th>categorical_0_ja</th>\n",
              "      <th>categorical_0_cd</th>\n",
              "      <th>categorical_0_a</th>\n",
              "      <th>categorical_0_j</th>\n",
              "      <th>categorical_0_hg</th>\n",
              "      <th>categorical_0_cg</th>\n",
              "      <th>categorical_0_gj</th>\n",
              "      <th>...</th>\n",
              "      <th>categorical_9_dc</th>\n",
              "      <th>categorical_9_ij</th>\n",
              "      <th>categorical_9_j</th>\n",
              "      <th>categorical_9_df</th>\n",
              "      <th>categorical_9_ef</th>\n",
              "      <th>categorical_9_i</th>\n",
              "      <th>categorical_9_gb</th>\n",
              "      <th>categorical_9_h</th>\n",
              "      <th>categorical_9_hg</th>\n",
              "      <th>categorical_9_jj</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>134</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>608</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>38</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>869</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>915</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1001</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1283</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>304</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>863</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>10 rows × 999 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      categorical_0_dd  categorical_0_ba  categorical_0_df  categorical_0_ja  \\\n",
              "134                  0                 0                 0                 0   \n",
              "608                  0                 0                 0                 0   \n",
              "38                   0                 0                 0                 0   \n",
              "869                  0                 0                 0                 0   \n",
              "915                  0                 0                 0                 0   \n",
              "1001                 0                 0                 0                 0   \n",
              "20                   0                 0                 0                 0   \n",
              "1283                 0                 0                 0                 0   \n",
              "304                  0                 0                 0                 0   \n",
              "863                  0                 0                 0                 0   \n",
              "\n",
              "      categorical_0_cd  categorical_0_a  categorical_0_j  categorical_0_hg  \\\n",
              "134                  0                0                0                 0   \n",
              "608                  0                0                0                 0   \n",
              "38                   0                0                0                 0   \n",
              "869                  0                0                0                 0   \n",
              "915                  0                0                0                 0   \n",
              "1001                 0                1                0                 0   \n",
              "20                   0                0                0                 0   \n",
              "1283                 0                0                0                 0   \n",
              "304                  0                0                0                 0   \n",
              "863                  0                0                0                 0   \n",
              "\n",
              "      categorical_0_cg  categorical_0_gj        ...         categorical_9_dc  \\\n",
              "134                  0                 0        ...                        0   \n",
              "608                  0                 0        ...                        0   \n",
              "38                   0                 0        ...                        0   \n",
              "869                  0                 0        ...                        0   \n",
              "915                  0                 0        ...                        0   \n",
              "1001                 0                 0        ...                        0   \n",
              "20                   0                 0        ...                        0   \n",
              "1283                 0                 0        ...                        0   \n",
              "304                  0                 0        ...                        0   \n",
              "863                  0                 0        ...                        0   \n",
              "\n",
              "      categorical_9_ij  categorical_9_j  categorical_9_df  categorical_9_ef  \\\n",
              "134                  0                0                 0                 0   \n",
              "608                  0                0                 0                 0   \n",
              "38                   0                0                 0                 0   \n",
              "869                  0                0                 0                 0   \n",
              "915                  0                0                 0                 0   \n",
              "1001                 0                0                 0                 0   \n",
              "20                   0                0                 0                 0   \n",
              "1283                 0                0                 0                 0   \n",
              "304                  0                0                 0                 0   \n",
              "863                  0                0                 0                 0   \n",
              "\n",
              "      categorical_9_i  categorical_9_gb  categorical_9_h  categorical_9_hg  \\\n",
              "134                 0                 0                0                 0   \n",
              "608                 0                 0                0                 0   \n",
              "38                  0                 0                0                 0   \n",
              "869                 0                 0                0                 0   \n",
              "915                 0                 0                0                 0   \n",
              "1001                0                 0                0                 0   \n",
              "20                  0                 0                0                 0   \n",
              "1283                0                 0                0                 0   \n",
              "304                 0                 0                0                 0   \n",
              "863                 0                 0                0                 0   \n",
              "\n",
              "      categorical_9_jj  \n",
              "134                  0  \n",
              "608                  0  \n",
              "38                   0  \n",
              "869                  0  \n",
              "915                  0  \n",
              "1001                 0  \n",
              "20                   0  \n",
              "1283                 0  \n",
              "304                  1  \n",
              "863                  0  \n",
              "\n",
              "[10 rows x 999 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 804
        }
      ]
    },
    {
      "metadata": {
        "id": "8HD4uwYjNbas",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We can fit the same model with the one-hot encoded data as we fit to the label-encoded data."
      ]
    },
    {
      "metadata": {
        "id": "-Bxu2FRyfqKb",
        "colab_type": "code",
        "outputId": "6a800c8d-7182-40ff-ae1a-ed9dcd0541e6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "# Regression model\n",
        "model = Pipeline([\n",
        "    ('encoder', OneHotEncoder()),\n",
        "    ('scaler', StandardScaler()),\n",
        "    ('imputer', SimpleImputer(strategy='mean')),\n",
        "    ('regressor', Regressor)\n",
        "])\n",
        "\n",
        "# Cross-validated MAE\n",
        "scores = cross_val_score(model, X_train, y_train, \n",
        "                         cv=3, scoring=mae_scorer)\n",
        "print('Cross-validated MAE:', scores.mean(), '+/-', scores.std())"
      ],
      "execution_count": 805,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cross-validated MAE: 2.643262604528567 +/- 0.04410142304588674\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Hu8cQWbsOHHI",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "TODO: much better!"
      ]
    },
    {
      "metadata": {
        "id": "2zvPONNFbp4v",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "TODO: compare CV performance to validation performance"
      ]
    },
    {
      "metadata": {
        "id": "T5wvQe0da6_S",
        "colab_type": "code",
        "outputId": "90159768-0f36-4dcb-eb10-aa2481c0bef0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "# MAE on test data\n",
        "model.fit(X_train, y_train)\n",
        "y_pred = model.predict(X_test)\n",
        "test_mae = mean_absolute_error(y_test, y_pred)\n",
        "print('Validation MAE:', test_mae)"
      ],
      "execution_count": 806,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Validation MAE: 2.5913815779421268\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "pJR4JtnALzk7",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Target Encoding\n",
        "\n",
        "TODO: target encoding replaces categorical vals w/ the mean of the target for that category"
      ]
    },
    {
      "metadata": {
        "id": "eLTfbgi02ram",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class TargetEncoder(BaseEstimator, TransformerMixin):\n",
        "    \"\"\"Target encoder.\n",
        "    \n",
        "    Replaces categorical column(s) with the mean target value for\n",
        "    each category.\n",
        "\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, cols=None):\n",
        "        \"\"\"Target encoder\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        cols : list of str\n",
        "            Columns to target encode.  Default is to target encode all \n",
        "            categorical columns in the DataFrame.\n",
        "        \"\"\"\n",
        "        if isinstance(cols, str):\n",
        "            self.cols = [cols]\n",
        "        else:\n",
        "            self.cols = cols\n",
        "        \n",
        "        \n",
        "    def fit(self, X, y):\n",
        "        \"\"\"Fit target encoder to X and y\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        X : pandas DataFrame, shape [n_samples, n_columns]\n",
        "            DataFrame containing columns to encode\n",
        "        y : pandas Series, shape = [n_samples]\n",
        "            Target values.\n",
        "            \n",
        "        Returns\n",
        "        -------\n",
        "        self : encoder\n",
        "            Returns self.\n",
        "        \"\"\"\n",
        "        \n",
        "        # Encode all categorical cols by default\n",
        "        if self.cols is None:\n",
        "            self.cols = [col for col in X if str(X[col].dtype)=='object']\n",
        "\n",
        "        # Check columns are in X\n",
        "        for col in self.cols:\n",
        "            if col not in X:\n",
        "                raise ValueError('Column \\''+col+'\\' not in X')\n",
        "\n",
        "        # Encode each element of each column\n",
        "        self.maps = dict() #dict to store map for each column\n",
        "        for col in self.cols:\n",
        "            tmap = dict()\n",
        "            uniques = X[col].unique()\n",
        "            for unique in uniques:\n",
        "                tmap[unique] = y[X[col]==unique].mean()\n",
        "            self.maps[col] = tmap\n",
        "            \n",
        "        return self\n",
        "\n",
        "        \n",
        "    def transform(self, X, y=None):\n",
        "        \"\"\"Perform the target encoding transformation.\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        X : pandas DataFrame, shape [n_samples, n_columns]\n",
        "            DataFrame containing columns to encode\n",
        "            \n",
        "        Returns\n",
        "        -------\n",
        "        pandas DataFrame\n",
        "            Input DataFrame with transformed columns\n",
        "        \"\"\"\n",
        "        Xo = X.copy()\n",
        "        for col, tmap in self.maps.items():\n",
        "            vals = np.full(X.shape[0], np.nan)\n",
        "            for val, mean_target in tmap.items():\n",
        "                vals[X[col]==val] = mean_target\n",
        "            Xo[col] = vals\n",
        "        return Xo\n",
        "            \n",
        "            \n",
        "    def fit_transform(self, X, y=None):\n",
        "        \"\"\"Fit and transform the data via target encoding.\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        X : pandas DataFrame, shape [n_samples, n_columns]\n",
        "            DataFrame containing columns to encode\n",
        "        y : pandas Series, shape = [n_samples]\n",
        "            Target values (required!).\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        pandas DataFrame\n",
        "            Input DataFrame with transformed columns\n",
        "        \"\"\"\n",
        "        return self.fit(X, y).transform(X, y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_S8jg9gZVvrV",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "TODO: now we can replace each categorical value with a continuous vaule corresponding to the mean of the target value for that category."
      ]
    },
    {
      "metadata": {
        "id": "G3Q_B2N5V2ZN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 379
        },
        "outputId": "5e6662c9-6f60-4c4c-9918-eb9125e1c24c"
      },
      "cell_type": "code",
      "source": [
        "# Target encode the categorical data\n",
        "te = TargetEncoder()\n",
        "X_target_encoded = te.fit_transform(X_train, y_train)\n",
        "X_target_encoded.sample(10)"
      ],
      "execution_count": 808,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>categorical_0</th>\n",
              "      <th>categorical_1</th>\n",
              "      <th>categorical_2</th>\n",
              "      <th>categorical_3</th>\n",
              "      <th>categorical_4</th>\n",
              "      <th>categorical_5</th>\n",
              "      <th>categorical_6</th>\n",
              "      <th>categorical_7</th>\n",
              "      <th>categorical_8</th>\n",
              "      <th>categorical_9</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1595</th>\n",
              "      <td>1.164802</td>\n",
              "      <td>-3.096685</td>\n",
              "      <td>0.713228</td>\n",
              "      <td>0.553015</td>\n",
              "      <td>2.240036</td>\n",
              "      <td>2.090636</td>\n",
              "      <td>0.669343</td>\n",
              "      <td>1.353607</td>\n",
              "      <td>1.366611</td>\n",
              "      <td>1.868270</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>0.527923</td>\n",
              "      <td>-1.784961</td>\n",
              "      <td>-2.289389</td>\n",
              "      <td>0.979364</td>\n",
              "      <td>-1.826264</td>\n",
              "      <td>-2.560666</td>\n",
              "      <td>0.670530</td>\n",
              "      <td>-0.748978</td>\n",
              "      <td>-0.858190</td>\n",
              "      <td>-0.058751</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>811</th>\n",
              "      <td>-2.390536</td>\n",
              "      <td>-0.523276</td>\n",
              "      <td>0.355815</td>\n",
              "      <td>1.182444</td>\n",
              "      <td>-0.404261</td>\n",
              "      <td>0.422867</td>\n",
              "      <td>-0.533822</td>\n",
              "      <td>1.721623</td>\n",
              "      <td>-1.334274</td>\n",
              "      <td>0.414638</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1230</th>\n",
              "      <td>1.298141</td>\n",
              "      <td>-0.094858</td>\n",
              "      <td>3.047039</td>\n",
              "      <td>0.290850</td>\n",
              "      <td>-0.200190</td>\n",
              "      <td>0.268334</td>\n",
              "      <td>0.077880</td>\n",
              "      <td>-1.474255</td>\n",
              "      <td>-0.386031</td>\n",
              "      <td>0.812788</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>717</th>\n",
              "      <td>-2.154734</td>\n",
              "      <td>-1.784961</td>\n",
              "      <td>-1.353943</td>\n",
              "      <td>0.745452</td>\n",
              "      <td>0.296738</td>\n",
              "      <td>-0.664256</td>\n",
              "      <td>-0.868004</td>\n",
              "      <td>-0.124779</td>\n",
              "      <td>-0.919462</td>\n",
              "      <td>-1.472477</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1347</th>\n",
              "      <td>0.387557</td>\n",
              "      <td>-1.653500</td>\n",
              "      <td>2.918879</td>\n",
              "      <td>0.704421</td>\n",
              "      <td>1.278560</td>\n",
              "      <td>-1.290959</td>\n",
              "      <td>0.670530</td>\n",
              "      <td>-0.804899</td>\n",
              "      <td>0.375242</td>\n",
              "      <td>3.206202</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1305</th>\n",
              "      <td>1.944669</td>\n",
              "      <td>0.121671</td>\n",
              "      <td>3.259884</td>\n",
              "      <td>0.745452</td>\n",
              "      <td>4.106555</td>\n",
              "      <td>-0.873544</td>\n",
              "      <td>0.530320</td>\n",
              "      <td>0.775188</td>\n",
              "      <td>3.076566</td>\n",
              "      <td>1.317814</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1222</th>\n",
              "      <td>-0.424381</td>\n",
              "      <td>-2.362084</td>\n",
              "      <td>-0.693528</td>\n",
              "      <td>1.894725</td>\n",
              "      <td>-1.826264</td>\n",
              "      <td>2.152663</td>\n",
              "      <td>-0.610215</td>\n",
              "      <td>0.058776</td>\n",
              "      <td>1.851855</td>\n",
              "      <td>-0.333716</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>232</th>\n",
              "      <td>0.650529</td>\n",
              "      <td>0.062263</td>\n",
              "      <td>3.259884</td>\n",
              "      <td>4.427027</td>\n",
              "      <td>-1.044957</td>\n",
              "      <td>0.898197</td>\n",
              "      <td>1.537396</td>\n",
              "      <td>1.256941</td>\n",
              "      <td>0.570482</td>\n",
              "      <td>0.399783</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1907</th>\n",
              "      <td>0.848548</td>\n",
              "      <td>1.587945</td>\n",
              "      <td>0.984121</td>\n",
              "      <td>1.894725</td>\n",
              "      <td>1.642278</td>\n",
              "      <td>-0.032045</td>\n",
              "      <td>-4.923657</td>\n",
              "      <td>-1.662882</td>\n",
              "      <td>0.683778</td>\n",
              "      <td>-1.002601</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      categorical_0  categorical_1  categorical_2  categorical_3  \\\n",
              "1595       1.164802      -3.096685       0.713228       0.553015   \n",
              "17         0.527923      -1.784961      -2.289389       0.979364   \n",
              "811       -2.390536      -0.523276       0.355815       1.182444   \n",
              "1230       1.298141      -0.094858       3.047039       0.290850   \n",
              "717       -2.154734      -1.784961      -1.353943       0.745452   \n",
              "1347       0.387557      -1.653500       2.918879       0.704421   \n",
              "1305       1.944669       0.121671       3.259884       0.745452   \n",
              "1222      -0.424381      -2.362084      -0.693528       1.894725   \n",
              "232        0.650529       0.062263       3.259884       4.427027   \n",
              "1907       0.848548       1.587945       0.984121       1.894725   \n",
              "\n",
              "      categorical_4  categorical_5  categorical_6  categorical_7  \\\n",
              "1595       2.240036       2.090636       0.669343       1.353607   \n",
              "17        -1.826264      -2.560666       0.670530      -0.748978   \n",
              "811       -0.404261       0.422867      -0.533822       1.721623   \n",
              "1230      -0.200190       0.268334       0.077880      -1.474255   \n",
              "717        0.296738      -0.664256      -0.868004      -0.124779   \n",
              "1347       1.278560      -1.290959       0.670530      -0.804899   \n",
              "1305       4.106555      -0.873544       0.530320       0.775188   \n",
              "1222      -1.826264       2.152663      -0.610215       0.058776   \n",
              "232       -1.044957       0.898197       1.537396       1.256941   \n",
              "1907       1.642278      -0.032045      -4.923657      -1.662882   \n",
              "\n",
              "      categorical_8  categorical_9  \n",
              "1595       1.366611       1.868270  \n",
              "17        -0.858190      -0.058751  \n",
              "811       -1.334274       0.414638  \n",
              "1230      -0.386031       0.812788  \n",
              "717       -0.919462      -1.472477  \n",
              "1347       0.375242       3.206202  \n",
              "1305       3.076566       1.317814  \n",
              "1222       1.851855      -0.333716  \n",
              "232        0.570482       0.399783  \n",
              "1907       0.683778      -1.002601  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 808
        }
      ]
    },
    {
      "metadata": {
        "id": "SRgdJww0Zj0N",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "TODO: note that each column has exactly as many unique continuous values as it did categories.  This is because we've simply replaced the category with the mean target value for that category."
      ]
    },
    {
      "metadata": {
        "id": "QpKHqIysZvcm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "2016fbb2-b299-4525-f83c-e3666fb6645e"
      },
      "cell_type": "code",
      "source": [
        "X_target_encoded.nunique()"
      ],
      "execution_count": 809,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "categorical_0    100\n",
              "categorical_1    100\n",
              "categorical_2    100\n",
              "categorical_3    100\n",
              "categorical_4    100\n",
              "categorical_5    100\n",
              "categorical_6    100\n",
              "categorical_7     99\n",
              "categorical_8    100\n",
              "categorical_9    100\n",
              "dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 809
        }
      ]
    },
    {
      "metadata": {
        "id": "iTTtxLxnWEN8",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "TODO: then we can fit the same model as before..."
      ]
    },
    {
      "metadata": {
        "id": "q7j8DFVNWGiF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "77e3feb3-56cd-4bc5-c78c-d241c7638b90"
      },
      "cell_type": "code",
      "source": [
        "# Regression model\n",
        "model = Pipeline([\n",
        "    ('encoder', TargetEncoder()),\n",
        "    ('scaler', StandardScaler()),\n",
        "    ('imputer', SimpleImputer(strategy='mean')),\n",
        "    ('regressor', Regressor)\n",
        "])\n",
        "\n",
        "# Cross-validated MAE\n",
        "scores = cross_val_score(model, X_train, y_train, \n",
        "                         cv=3, scoring=mae_scorer)\n",
        "print('Cross-validated MAE:', scores.mean(), '+/-', scores.std())"
      ],
      "execution_count": 810,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cross-validated MAE: 2.25309372358836 +/- 0.03773355673767985\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "RA5txfaXcdcA",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "TODO: uh, in theory cross-validated performance will be better than validation...\n"
      ]
    },
    {
      "metadata": {
        "id": "Sq3DV6aIYMeE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "78272c9a-f567-4e59-a715-9dc0e85e5b01"
      },
      "cell_type": "code",
      "source": [
        "# MAE on test data\n",
        "model.fit(X_train, y_train)\n",
        "y_pred = model.predict(X_test)\n",
        "test_mae = mean_absolute_error(y_test, y_pred)\n",
        "print('Validation MAE:', test_mae)"
      ],
      "execution_count": 811,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Validation MAE: 2.0894385611402564\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "lTsmSMPyYPtV",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "TODO: uhhhh?"
      ]
    },
    {
      "metadata": {
        "id": "d_CvINwf2v9f",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Cross-Fold Target Encoding\n",
        "\n",
        "TODO: problems w/ just target encoding (your performance on the test data goes way down relative to your CV score), and why it happens (b/c you're causing data leakage by allowing i-th sample's y variable to effect the value of the i-th sample's X variable which you're encoding) \n",
        "\n",
        "TODO: this can be fixed w/ cross-fold target encoding"
      ]
    },
    {
      "metadata": {
        "id": "lls4RKG-2vZ4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class TargetEncoderCV(TargetEncoder):\n",
        "    \"\"\"Cross-validated target encoder.\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, n_splits=3, shuffle=True, cols=None):\n",
        "        \"\"\"Cross-validated target encoding for categorical features.\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        n_splits : int\n",
        "            Number of cross-validation splits. Default = 3.\n",
        "        shuffle : bool\n",
        "            Whether to shuffle the data when splitting into folds.\n",
        "        cols : list of str\n",
        "            Columns to target encode.\n",
        "        \"\"\"\n",
        "        self.n_splits = n_splits\n",
        "        self.shuffle = shuffle\n",
        "        self.cols = cols\n",
        "        \n",
        "\n",
        "    def fit(self, X, y):\n",
        "        \"\"\"Fit target encoder to X and y\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        X : pandas DataFrame, shape [n_samples, n_columns]\n",
        "            DataFrame containing columns to encode\n",
        "        y : pandas Series, shape = [n_samples]\n",
        "            Target values.\n",
        "            \n",
        "        Returns\n",
        "        -------\n",
        "        self : encoder\n",
        "            Returns self.\n",
        "        \"\"\"\n",
        "        self._target_encoder = TargetEncoder(cols=self.cols)\n",
        "        self._target_encoder.fit(X, y)\n",
        "        return self\n",
        "\n",
        "    \n",
        "    def transform(self, X, y=None):\n",
        "        \"\"\"Perform the target encoding transformation.\n",
        "\n",
        "        Uses cross-validated target encoding for the training fold, and uses\n",
        "        normal target encoding for the test fold.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : pandas DataFrame, shape [n_samples, n_columns]\n",
        "            DataFrame containing columns to encode\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        pandas DataFrame\n",
        "            Input DataFrame with transformed columns\n",
        "        \"\"\"\n",
        "\n",
        "        # Use target encoding from fit() if this is test data\n",
        "        if y is None:\n",
        "            return self._target_encoder.transform(X)\n",
        "\n",
        "        # Compute means for each fold\n",
        "        self._train_ix = []\n",
        "        self._test_ix = []\n",
        "        self._fit_tes = []\n",
        "        kf = KFold(n_splits=self.n_splits, shuffle=self.shuffle)\n",
        "        for train_ix, test_ix in kf.split(X):\n",
        "            self._train_ix.append(train_ix)\n",
        "            self._test_ix.append(test_ix)\n",
        "            te = TargetEncoder(cols=self.cols)\n",
        "            if isinstance(X, pd.DataFrame):\n",
        "                self._fit_tes.append(te.fit(X.iloc[train_ix,:],\n",
        "                                            y.iloc[train_ix]))\n",
        "            elif isinstance(X, np.ndarray):\n",
        "                self._fit_tes.append(te.fit(X[train_ix,:], y[train_ix]))\n",
        "            else:\n",
        "                raise TypeError('X must be DataFrame or ndarray')\n",
        "\n",
        "        # Apply means across folds\n",
        "        Xo = X.copy()\n",
        "        for ix in range(len(self._test_ix)):\n",
        "            test_ix = self._test_ix[ix]\n",
        "            if isinstance(X, pd.DataFrame):\n",
        "                Xo.iloc[test_ix,:] = self._fit_tes[ix].transform(X.iloc[test_ix,:])\n",
        "            elif isinstance(X, np.ndarray):\n",
        "                Xo[test_ix,:] = self._fit_tes[ix].transform(X[test_ix,:])\n",
        "            else:\n",
        "                raise TypeError('X must be DataFrame or ndarray')\n",
        "        return Xo\n",
        "\n",
        "            \n",
        "    def fit_transform(self, X, y=None):\n",
        "        \"\"\"Fit and transform the data via target encoding.\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        X : pandas DataFrame, shape [n_samples, n_columns]\n",
        "            DataFrame containing columns to encode\n",
        "        y : pandas Series, shape = [n_samples]\n",
        "            Target values (required!).\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        pandas DataFrame\n",
        "            Input DataFrame with transformed columns\n",
        "        \"\"\"\n",
        "        return self.fit(X, y).transform(X, y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2e3eQ8-6Z1It",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "With this encoder, we can convert the categories into continuous values, just like we did with the naive target encoding."
      ]
    },
    {
      "metadata": {
        "id": "FDlvSOS5Z12A",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 379
        },
        "outputId": "06eb2f68-b9cb-4d79-eea9-7b39f3ece079"
      },
      "cell_type": "code",
      "source": [
        "# Cross-fold Target encode the categorical data\n",
        "te = TargetEncoderCV()\n",
        "X_target_encoded_cv = te.fit_transform(X_train, y_train)\n",
        "X_target_encoded_cv.sample(10)"
      ],
      "execution_count": 813,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>categorical_0</th>\n",
              "      <th>categorical_1</th>\n",
              "      <th>categorical_2</th>\n",
              "      <th>categorical_3</th>\n",
              "      <th>categorical_4</th>\n",
              "      <th>categorical_5</th>\n",
              "      <th>categorical_6</th>\n",
              "      <th>categorical_7</th>\n",
              "      <th>categorical_8</th>\n",
              "      <th>categorical_9</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>208</th>\n",
              "      <td>0.486431</td>\n",
              "      <td>-1.86036</td>\n",
              "      <td>-2.3132</td>\n",
              "      <td>0.976029</td>\n",
              "      <td>0.516028</td>\n",
              "      <td>-0.160255</td>\n",
              "      <td>1.40905</td>\n",
              "      <td>-0.372855</td>\n",
              "      <td>0.396788</td>\n",
              "      <td>0.781938</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1403</th>\n",
              "      <td>0.887295</td>\n",
              "      <td>2.17109</td>\n",
              "      <td>0.97772</td>\n",
              "      <td>1.42101</td>\n",
              "      <td>-0.701869</td>\n",
              "      <td>-1.07543</td>\n",
              "      <td>1.44525</td>\n",
              "      <td>-1.05794</td>\n",
              "      <td>-0.195603</td>\n",
              "      <td>0.0106026</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>983</th>\n",
              "      <td>-1.79535</td>\n",
              "      <td>-2.01714</td>\n",
              "      <td>-4.19868</td>\n",
              "      <td>-3.96676</td>\n",
              "      <td>-1.89881</td>\n",
              "      <td>-2.1041</td>\n",
              "      <td>2.72997</td>\n",
              "      <td>-0.903507</td>\n",
              "      <td>-0.757347</td>\n",
              "      <td>-0.128342</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>227</th>\n",
              "      <td>0.72623</td>\n",
              "      <td>-1.09823</td>\n",
              "      <td>-0.99161</td>\n",
              "      <td>6.06382</td>\n",
              "      <td>2.5456</td>\n",
              "      <td>0.38748</td>\n",
              "      <td>0.631625</td>\n",
              "      <td>-1.81481</td>\n",
              "      <td>0.687161</td>\n",
              "      <td>-0.459957</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>568</th>\n",
              "      <td>-1.50775</td>\n",
              "      <td>0.175597</td>\n",
              "      <td>2.96704</td>\n",
              "      <td>1.41403</td>\n",
              "      <td>0.758955</td>\n",
              "      <td>-0.954333</td>\n",
              "      <td>-0.248667</td>\n",
              "      <td>0.386701</td>\n",
              "      <td>0.560203</td>\n",
              "      <td>1.41309</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>435</th>\n",
              "      <td>2.731</td>\n",
              "      <td>2.38543</td>\n",
              "      <td>3.20024</td>\n",
              "      <td>-0.946659</td>\n",
              "      <td>-0.398711</td>\n",
              "      <td>-1.03573</td>\n",
              "      <td>-0.937953</td>\n",
              "      <td>-0.528115</td>\n",
              "      <td>-2.27535</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1887</th>\n",
              "      <td>0.587754</td>\n",
              "      <td>1.45873</td>\n",
              "      <td>-3.15958</td>\n",
              "      <td>2.19803</td>\n",
              "      <td>-0.303705</td>\n",
              "      <td>-0.602943</td>\n",
              "      <td>-2.01734</td>\n",
              "      <td>0.810857</td>\n",
              "      <td>-1.21283</td>\n",
              "      <td>0.230463</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>346</th>\n",
              "      <td>-1.15319</td>\n",
              "      <td>-2.38764</td>\n",
              "      <td>2.72171</td>\n",
              "      <td>0.452571</td>\n",
              "      <td>4.33045</td>\n",
              "      <td>2.24501</td>\n",
              "      <td>3.72833</td>\n",
              "      <td>1.09455</td>\n",
              "      <td>-0.999654</td>\n",
              "      <td>-0.0430413</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1238</th>\n",
              "      <td>2.56855</td>\n",
              "      <td>0.502569</td>\n",
              "      <td>-0.315665</td>\n",
              "      <td>0.528216</td>\n",
              "      <td>-0.0701429</td>\n",
              "      <td>4.01609</td>\n",
              "      <td>0.680515</td>\n",
              "      <td>0.877728</td>\n",
              "      <td>-2.24616</td>\n",
              "      <td>2.48207</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1753</th>\n",
              "      <td>0.420239</td>\n",
              "      <td>-2.57994</td>\n",
              "      <td>1.12341</td>\n",
              "      <td>0.613835</td>\n",
              "      <td>1.99796</td>\n",
              "      <td>0.930592</td>\n",
              "      <td>-1.06842</td>\n",
              "      <td>3.76682</td>\n",
              "      <td>2.75065</td>\n",
              "      <td>-1.14513</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     categorical_0 categorical_1 categorical_2 categorical_3 categorical_4  \\\n",
              "208       0.486431      -1.86036       -2.3132      0.976029      0.516028   \n",
              "1403      0.887295       2.17109       0.97772       1.42101     -0.701869   \n",
              "983       -1.79535      -2.01714      -4.19868      -3.96676      -1.89881   \n",
              "227        0.72623      -1.09823      -0.99161       6.06382        2.5456   \n",
              "568       -1.50775      0.175597       2.96704       1.41403      0.758955   \n",
              "435          2.731       2.38543       3.20024     -0.946659     -0.398711   \n",
              "1887      0.587754       1.45873      -3.15958       2.19803     -0.303705   \n",
              "346       -1.15319      -2.38764       2.72171      0.452571       4.33045   \n",
              "1238       2.56855      0.502569     -0.315665      0.528216    -0.0701429   \n",
              "1753      0.420239      -2.57994       1.12341      0.613835       1.99796   \n",
              "\n",
              "     categorical_5 categorical_6 categorical_7 categorical_8 categorical_9  \n",
              "208      -0.160255       1.40905     -0.372855      0.396788      0.781938  \n",
              "1403      -1.07543       1.44525      -1.05794     -0.195603     0.0106026  \n",
              "983        -2.1041       2.72997     -0.903507     -0.757347     -0.128342  \n",
              "227        0.38748      0.631625      -1.81481      0.687161     -0.459957  \n",
              "568      -0.954333     -0.248667      0.386701      0.560203       1.41309  \n",
              "435       -1.03573     -0.937953     -0.528115      -2.27535           NaN  \n",
              "1887     -0.602943      -2.01734      0.810857      -1.21283      0.230463  \n",
              "346        2.24501       3.72833       1.09455     -0.999654    -0.0430413  \n",
              "1238       4.01609      0.680515      0.877728      -2.24616       2.48207  \n",
              "1753      0.930592      -1.06842       3.76682       2.75065      -1.14513  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 813
        }
      ]
    },
    {
      "metadata": {
        "id": "uZrZLdx-Z81V",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "However, now we have more unique continuous values in each column than we did categories, because we've target-encoded the categories separately for each fold."
      ]
    },
    {
      "metadata": {
        "id": "vuqXaUqIaEXl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "f45eecdb-e550-4ecb-cbdf-dd09bfe7e02d"
      },
      "cell_type": "code",
      "source": [
        "X_target_encoded_cv.nunique()"
      ],
      "execution_count": 814,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "categorical_0    289\n",
              "categorical_1    292\n",
              "categorical_2    288\n",
              "categorical_3    294\n",
              "categorical_4    289\n",
              "categorical_5    291\n",
              "categorical_6    289\n",
              "categorical_7    286\n",
              "categorical_8    284\n",
              "categorical_9    285\n",
              "dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 814
        }
      ]
    },
    {
      "metadata": {
        "id": "I1dys8eHxYAq",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "TODO: then we can fit the same model as before, but w/ cross-fold target encoding..."
      ]
    },
    {
      "metadata": {
        "id": "lOn6xCnqxZ3T",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "f0bdccd1-e3a7-4da2-b863-3154430ee810"
      },
      "cell_type": "code",
      "source": [
        "# Regression model\n",
        "model = Pipeline([\n",
        "    ('encoder', TargetEncoderCV()),\n",
        "    ('scaler', StandardScaler()),\n",
        "    ('imputer', SimpleImputer(strategy='mean')),\n",
        "    ('regressor', Regressor)\n",
        "])\n",
        "\n",
        "# Cross-validated MAE\n",
        "scores = cross_val_score(model, X_train, y_train, \n",
        "                         cv=3, scoring=mae_scorer)\n",
        "print('Cross-validated MAE:', scores.mean(), '+/-', scores.std())"
      ],
      "execution_count": 815,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cross-validated MAE: 2.5967589317192563 +/- 0.051025794807774257\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "GCjhesDoxtfK",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "TODO: and now the validation performance is consistent with the cross-validated performance"
      ]
    },
    {
      "metadata": {
        "id": "uIj56K0Hx8A7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "b5b3205f-1e5d-40d6-aa09-bdcdce7789cf"
      },
      "cell_type": "code",
      "source": [
        "# MAE on test data\n",
        "model.fit(X_train, y_train)\n",
        "y_pred = model.predict(X_test)\n",
        "test_mae = mean_absolute_error(y_test, y_pred)\n",
        "print('Validation MAE:', test_mae)"
      ],
      "execution_count": 816,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Validation MAE: 2.4734128174290904\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "hNryKyVCM0cr",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Dependence on Number of Categories\n",
        "\n",
        "TODO: not using cross-fold TE is worse when there are more categories.\n",
        "\n",
        "TODO: but if you have a very small number of categories, you may be better off using one-hot (compare mae's as a fn of #categories/#samples)"
      ]
    },
    {
      "metadata": {
        "id": "nACpTFNGM-57",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "pXAZQCjmNiYz",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Effect of Interactions\n",
        "\n",
        "TODO: target encoding doesn't capture interactions very well, which one-hot does (in theory?  simulate data w/ important interaction effects and see if it does better than target encoder)"
      ]
    },
    {
      "metadata": {
        "id": "a-pdSDv8cvZH",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# TODO: add some sort of interactions param to make_categorical_regression func?"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}