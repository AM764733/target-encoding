{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Target Encoding",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "wlMOSchK2noH",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Target Encoding\n",
        "\n",
        "TODO: intro about ways to encode categorical cols: label, one-hot, and entity embbeddings (if you were using a neural net or gradient descent-based method, but can't do that if you want to use say xgboost).  Problem w/ label encoding is doesn't work at all w/ non-ordinal features w/ a high # of categories (or at all at all w/ a linear model).  Problem w/ one-hot is that it can create a crazy number of columns.  \n",
        "\n",
        "TODO: One solution is to use target encoding, where you replace each category with the mean target value for samples having that category.  However, blindly using target encoding can allow data leakage, leading to very poor cross-validated or validation performance!  To fix that problem, have to do cross-fold target encoding.\n",
        "\n",
        "TODO: but even with cross-fold target encoding, there are situations where you'd be better off with one-hot or other encoding methods: one-hot is usually better in situations with few categories, and one-hot is definitely better when there are important interaction effects.\n",
        "\n",
        "In this post we'll evaluate different encoding schemes, build a cross-fold target encoder to mitigate the drawbacks of the naive target encoder, and determine how the performance of predictive models change based on the type of category encoding used, the number of categories in the dataset, and the presence of interaction effects. \n",
        "\n",
        "TODO: outline\n",
        "\n",
        "First let's import the packages we'll be using."
      ]
    },
    {
      "metadata": {
        "id": "fXS8EKjT2o7n",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn.metrics import mean_absolute_error, make_scorer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.model_selection import cross_val_score, cross_val_predict\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from xgboost import XGBRegressor"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8LQUOJ8D2q6f",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Data\n",
        "\n",
        "To evaluate the effectiveness of different encoding algorithms, we'll want to be able to generate data with different numbers of samples, features, and categories.  Let's make a function to generate categorical datasets, which allows us to set these different aspects of the data."
      ]
    },
    {
      "metadata": {
        "id": "31A0dwV-Lohj",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def make_categorical_regression(n_samples=100,\n",
        "                                n_features=10,\n",
        "                                n_informative=10,\n",
        "                                n_categories=10,\n",
        "                                noise=1.0):\n",
        "    \"\"\"Generate a regression problem with only categorical features.\n",
        "  \n",
        "    Parameters\n",
        "    ----------\n",
        "    n_samples : int\n",
        "        Number of samples to generate\n",
        "    n_features : int\n",
        "        Number of categorical features to generate\n",
        "    n_informative : int\n",
        "        Number of features which carry information about the target\n",
        "    n_categories : int or list or ndarray\n",
        "        Number of categories per feature.\n",
        "    noise : float\n",
        "        Noise to add to target\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    X : pandas DataFrame of shape (n_samples, n_features)\n",
        "        Categorical features.\n",
        "    y : pandas Series of shape (n_samples,)\n",
        "        Target variable.\n",
        "    \"\"\"\n",
        "\n",
        "    # Check inputs\n",
        "    if not isinstance(n_samples, int):\n",
        "        raise TypeError('n_samples must be an int')\n",
        "    if n_samples < 2:\n",
        "        raise ValueError('n_samples must be one or greater')\n",
        "    if not isinstance(n_features, int):\n",
        "        raise TypeError('n_features must be an int')\n",
        "    if n_features < 2:\n",
        "        raise ValueError('n_features must be one or greater')\n",
        "    if not isinstance(n_informative, int):\n",
        "        raise TypeError('n_informative must be an int')\n",
        "    if n_informative < 2:\n",
        "        raise ValueError('n_informative must be one or greater')\n",
        "    if not isinstance(n_categories, int):\n",
        "        raise TypeError('n_categories must be an int')\n",
        "    if n_categories < 2:\n",
        "        raise ValueError('n_categories must be one or greater')\n",
        "    if not isinstance(noise, float):\n",
        "        raise TypeError('noise must be a float')\n",
        "    if noise < 0:\n",
        "        raise ValueError('noise must be positive')\n",
        "        \n",
        "    # Generate random categorical data\n",
        "    categories = np.random.randint(n_categories,\n",
        "                                   size=(n_samples, n_features))\n",
        "    \n",
        "    # Generate random values for each category\n",
        "    cat_vals = np.random.randn(n_categories, n_features)\n",
        "    \n",
        "    # Set non-informative columns' effect to 0\n",
        "    cat_vals[:,:(n_features-n_informative)] = 0\n",
        "    \n",
        "    # Compute target variable from those categories and their values\n",
        "    y = np.zeros(n_samples)\n",
        "    for iC in range(n_features):\n",
        "      y += cat_vals[categories[:,iC], iC]\n",
        "    \n",
        "    # Add noise\n",
        "    y += noise*np.random.rand(n_samples)\n",
        "    \n",
        "    # Generate dataframe from categories\n",
        "    cat_strs = [''.join([chr(ord(c)+49) for c in str(n)]) \n",
        "                for n in range(n_categories)]\n",
        "    X = pd.DataFrame()\n",
        "    for iC in range(n_features):\n",
        "        col_str = 'feature_'+str(iC)\n",
        "        X[col_str] = [cat_strs[i] for i in categories[:,iC]]\n",
        "            \n",
        "    # Generate series from target\n",
        "    y = pd.Series(data=y, index=X.index)\n",
        "    \n",
        "    # Return features and target\n",
        "    return X, y"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "AwugyiOT3xgn",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Now, we can easily generate data to test our encoders on:"
      ]
    },
    {
      "metadata": {
        "id": "RnHHmJlIw-AK",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Generate categorical data and target\n",
        "N = 10000\n",
        "X, y = make_categorical_regression(n_samples=N,\n",
        "                                   n_features=10,\n",
        "                                   n_categories=5)\n",
        "\n",
        "# Split into test and training data\n",
        "N2 = int(N/2)\n",
        "X_train = X.iloc[:N2, :]\n",
        "y_train = y[:N2]\n",
        "X_test = X.iloc[N2:, :]\n",
        "y_test = y[N2:]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "XP8jiuvu4InP",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The features are all categorical (stored as \"objects\"):"
      ]
    },
    {
      "metadata": {
        "id": "iBiFTq5S4Mtn",
        "colab_type": "code",
        "outputId": "17bb2857-d875-4892-80b2-e0cd7e801fed",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 379
        }
      },
      "cell_type": "code",
      "source": [
        "X_train.sample(10)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>feature_0</th>\n",
              "      <th>feature_1</th>\n",
              "      <th>feature_2</th>\n",
              "      <th>feature_3</th>\n",
              "      <th>feature_4</th>\n",
              "      <th>feature_5</th>\n",
              "      <th>feature_6</th>\n",
              "      <th>feature_7</th>\n",
              "      <th>feature_8</th>\n",
              "      <th>feature_9</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>4998</th>\n",
              "      <td>b</td>\n",
              "      <td>a</td>\n",
              "      <td>d</td>\n",
              "      <td>e</td>\n",
              "      <td>e</td>\n",
              "      <td>c</td>\n",
              "      <td>d</td>\n",
              "      <td>b</td>\n",
              "      <td>c</td>\n",
              "      <td>c</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2859</th>\n",
              "      <td>d</td>\n",
              "      <td>d</td>\n",
              "      <td>c</td>\n",
              "      <td>c</td>\n",
              "      <td>d</td>\n",
              "      <td>d</td>\n",
              "      <td>a</td>\n",
              "      <td>c</td>\n",
              "      <td>b</td>\n",
              "      <td>c</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4824</th>\n",
              "      <td>a</td>\n",
              "      <td>e</td>\n",
              "      <td>c</td>\n",
              "      <td>d</td>\n",
              "      <td>e</td>\n",
              "      <td>b</td>\n",
              "      <td>b</td>\n",
              "      <td>d</td>\n",
              "      <td>d</td>\n",
              "      <td>d</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>662</th>\n",
              "      <td>c</td>\n",
              "      <td>d</td>\n",
              "      <td>b</td>\n",
              "      <td>d</td>\n",
              "      <td>a</td>\n",
              "      <td>d</td>\n",
              "      <td>a</td>\n",
              "      <td>b</td>\n",
              "      <td>e</td>\n",
              "      <td>e</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1093</th>\n",
              "      <td>e</td>\n",
              "      <td>c</td>\n",
              "      <td>a</td>\n",
              "      <td>a</td>\n",
              "      <td>c</td>\n",
              "      <td>d</td>\n",
              "      <td>d</td>\n",
              "      <td>a</td>\n",
              "      <td>d</td>\n",
              "      <td>d</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1911</th>\n",
              "      <td>d</td>\n",
              "      <td>a</td>\n",
              "      <td>b</td>\n",
              "      <td>b</td>\n",
              "      <td>e</td>\n",
              "      <td>c</td>\n",
              "      <td>c</td>\n",
              "      <td>d</td>\n",
              "      <td>e</td>\n",
              "      <td>e</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2275</th>\n",
              "      <td>e</td>\n",
              "      <td>b</td>\n",
              "      <td>a</td>\n",
              "      <td>e</td>\n",
              "      <td>c</td>\n",
              "      <td>e</td>\n",
              "      <td>d</td>\n",
              "      <td>b</td>\n",
              "      <td>a</td>\n",
              "      <td>d</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3436</th>\n",
              "      <td>a</td>\n",
              "      <td>c</td>\n",
              "      <td>d</td>\n",
              "      <td>d</td>\n",
              "      <td>b</td>\n",
              "      <td>d</td>\n",
              "      <td>e</td>\n",
              "      <td>a</td>\n",
              "      <td>b</td>\n",
              "      <td>a</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1969</th>\n",
              "      <td>a</td>\n",
              "      <td>a</td>\n",
              "      <td>e</td>\n",
              "      <td>b</td>\n",
              "      <td>d</td>\n",
              "      <td>a</td>\n",
              "      <td>b</td>\n",
              "      <td>c</td>\n",
              "      <td>d</td>\n",
              "      <td>b</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1880</th>\n",
              "      <td>d</td>\n",
              "      <td>d</td>\n",
              "      <td>d</td>\n",
              "      <td>c</td>\n",
              "      <td>c</td>\n",
              "      <td>a</td>\n",
              "      <td>d</td>\n",
              "      <td>b</td>\n",
              "      <td>d</td>\n",
              "      <td>d</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     feature_0 feature_1 feature_2 feature_3 feature_4 feature_5 feature_6  \\\n",
              "4998         b         a         d         e         e         c         d   \n",
              "2859         d         d         c         c         d         d         a   \n",
              "4824         a         e         c         d         e         b         b   \n",
              "662          c         d         b         d         a         d         a   \n",
              "1093         e         c         a         a         c         d         d   \n",
              "1911         d         a         b         b         e         c         c   \n",
              "2275         e         b         a         e         c         e         d   \n",
              "3436         a         c         d         d         b         d         e   \n",
              "1969         a         a         e         b         d         a         b   \n",
              "1880         d         d         d         c         c         a         d   \n",
              "\n",
              "     feature_7 feature_8 feature_9  \n",
              "4998         b         c         c  \n",
              "2859         c         b         c  \n",
              "4824         d         d         d  \n",
              "662          b         e         e  \n",
              "1093         a         d         d  \n",
              "1911         d         e         e  \n",
              "2275         b         a         d  \n",
              "3436         a         b         a  \n",
              "1969         c         d         b  \n",
              "1880         b         d         d  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "metadata": {
        "id": "yxaiV1qjnpJb",
        "colab_type": "code",
        "outputId": "7ea01896-2fd0-4ea3-ff33-dd95b5d16005",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        }
      },
      "cell_type": "code",
      "source": [
        "X_train.info()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 5000 entries, 0 to 4999\n",
            "Data columns (total 10 columns):\n",
            "feature_0    5000 non-null object\n",
            "feature_1    5000 non-null object\n",
            "feature_2    5000 non-null object\n",
            "feature_3    5000 non-null object\n",
            "feature_4    5000 non-null object\n",
            "feature_5    5000 non-null object\n",
            "feature_6    5000 non-null object\n",
            "feature_7    5000 non-null object\n",
            "feature_8    5000 non-null object\n",
            "feature_9    5000 non-null object\n",
            "dtypes: object(10)\n",
            "memory usage: 390.7+ KB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Ckap_xbm-ZFo",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "But the target variable is continuous:"
      ]
    },
    {
      "metadata": {
        "id": "y_82h44JyBW7",
        "colab_type": "code",
        "outputId": "65c4ae8c-2cdf-458e-983b-8525427263b6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 364
        }
      },
      "cell_type": "code",
      "source": [
        "y_train.hist(bins=20)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7f3dfb1c0fd0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAeQAAAFKCAYAAADMuCxnAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 576x396 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "sH3ADlvjAbo6",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Let's see which category encoding scheme best allows us to predict the target variable!"
      ]
    },
    {
      "metadata": {
        "id": "7gsbU-a4ArVl",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Baseline\n",
        "\n",
        "For comparison, how well would we do if we just predicted the mean?  We'll use the mean absolute error as our performance metric."
      ]
    },
    {
      "metadata": {
        "id": "UTQOgHegAqhh",
        "colab_type": "code",
        "outputId": "3fabe953-3751-44d6-badc-5a2458dedc26",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "mean_absolute_error(y_train, \n",
        "                    np.full(y_train.shape[0], y_train.mean()))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2.4228639649417563"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "metadata": {
        "id": "tPKR7mZICTrT",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "So, we should definitely be shooting for a mean absolute error of less than 1.64!"
      ]
    },
    {
      "metadata": {
        "id": "F3spAaSlLozj",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Label Encoding\n",
        "\n",
        "TODO: prediction w/ simple label encoding, which is just replacing each unique category w/ a unique integer\n",
        "\n",
        "TODO: note that we could also use Scikit-learn's LabelEncoder"
      ]
    },
    {
      "metadata": {
        "id": "ZMGTkK-_Ltvr",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def label_encode(df, cols=None, inplace=False):\n",
        "    \"\"\"Convert categorical columns to unique integers.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    df : pandas DataFrame\n",
        "        DataFrame in which to convert columns\n",
        "    cols : list of str\n",
        "        List of columns to encode.\n",
        "        Default is to encode all the categorical columns in df.\n",
        "    inplace : bool\n",
        "        Whether to modify df as-is (True) or create a copy (False).\n",
        "        Default = False\n",
        "        \n",
        "    Returns\n",
        "    -------\n",
        "    dict\n",
        "        Mapping between categories and their integer labels.\n",
        "    \"\"\"\n",
        "    \n",
        "    # Modify in-place?\n",
        "    if not inplace:\n",
        "        df = df.copy()\n",
        "\n",
        "    # Do for all \"object\" columns if not specified\n",
        "    if cols is None:\n",
        "        cols = [col for col in df if str(df[col].dtype)=='object']\n",
        "    if len(cols) == 0:\n",
        "        return\n",
        "\n",
        "    # Make list if not\n",
        "    if isinstance(cols, str):\n",
        "        cols = [cols]\n",
        "\n",
        "    # Map each column\n",
        "    maps = dict()\n",
        "    for col in cols:\n",
        "\n",
        "        # Create the map from objects to integers\n",
        "        maps[col] = dict(zip(\n",
        "            df[col].values, \n",
        "            df[col].astype('category').cat.codes.values\n",
        "        ))\n",
        "\n",
        "        # Determine appropriate datatype\n",
        "        max_val = max(maps[col].values())\n",
        "        if df[col].isnull().any(): #nulls, so have to use float!\n",
        "            if max_val < 8388608:\n",
        "                dtype = 'float32'\n",
        "            else:\n",
        "                dtype = 'float64'\n",
        "        else:\n",
        "            if max_val < 256:\n",
        "                dtype = 'uint8'\n",
        "            elif max_val < 65536:\n",
        "                dtype = 'uint16'\n",
        "            elif max_val < 4294967296:\n",
        "                dtype = 'uint32'\n",
        "            else:\n",
        "                dtype = 'uint64'\n",
        "\n",
        "        # Map the column\n",
        "        df[col] = df[col].map(maps[col]).astype(dtype)\n",
        "\n",
        "    # Return the maps used\n",
        "    return df"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_Uen3yY-CpFX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Label-encode the categorical data\n",
        "X_label_encoded = label_encode(X_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "nDhwfsV5DDpj",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Now we've converted the categories to integers:"
      ]
    },
    {
      "metadata": {
        "id": "Y2YJidsuC0if",
        "colab_type": "code",
        "outputId": "ce924736-b1f5-4854-d441-d2cf6bc7f6e5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 379
        }
      },
      "cell_type": "code",
      "source": [
        "X_label_encoded.sample(10)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>feature_0</th>\n",
              "      <th>feature_1</th>\n",
              "      <th>feature_2</th>\n",
              "      <th>feature_3</th>\n",
              "      <th>feature_4</th>\n",
              "      <th>feature_5</th>\n",
              "      <th>feature_6</th>\n",
              "      <th>feature_7</th>\n",
              "      <th>feature_8</th>\n",
              "      <th>feature_9</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>3463</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3545</th>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2342</th>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4887</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>935</th>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2019</th>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2776</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2371</th>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4930</th>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4662</th>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      feature_0  feature_1  feature_2  feature_3  feature_4  feature_5  \\\n",
              "3463          0          1          1          4          0          4   \n",
              "3545          3          1          4          0          0          3   \n",
              "2342          4          3          0          2          3          1   \n",
              "4887          0          1          2          4          4          2   \n",
              "935           3          2          2          1          1          4   \n",
              "2019          2          2          1          4          4          4   \n",
              "2776          1          0          4          3          4          2   \n",
              "2371          2          3          2          1          0          4   \n",
              "4930          2          3          2          2          4          2   \n",
              "4662          3          0          3          1          2          3   \n",
              "\n",
              "      feature_6  feature_7  feature_8  feature_9  \n",
              "3463          3          3          0          4  \n",
              "3545          2          0          4          0  \n",
              "2342          0          3          4          2  \n",
              "4887          0          3          0          1  \n",
              "935           1          3          3          0  \n",
              "2019          3          2          0          1  \n",
              "2776          3          4          3          1  \n",
              "2371          2          4          4          4  \n",
              "4930          0          0          3          0  \n",
              "4662          2          2          4          0  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "metadata": {
        "id": "67LwHzQAW1Rl",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "But again, these integers aren't related to the categories in any meaningful way - aside from the fact that each unique integer corresponds to a unique category.\n",
        "\n",
        "TODO: then we can make a model to predict"
      ]
    },
    {
      "metadata": {
        "id": "u4Nibf2SW061",
        "colab_type": "code",
        "outputId": "74ed47db-f372-4e15-c036-0bde0c33b224",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 360
        }
      },
      "cell_type": "code",
      "source": [
        "# Regression model\n",
        "model = Pipeline([\n",
        "    ('scaler', StandardScaler()),\n",
        "    ('regressor', LinearRegression())\n",
        "    #('regressor', XGBRegressor())\n",
        "])\n",
        "\n",
        "# Cross-validated MSE\n",
        "mae_scorer = make_scorer(mean_absolute_error)\n",
        "scores = cross_val_score(model, X_label_encoded, y_train, \n",
        "                         cv=3, scoring=mae_scorer)\n",
        "print('Cross-validated MAE:', scores.mean(), '+/-', scores.std())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cross-validated MAE: 2.0256164635158656 +/- 0.0302061333016236\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/preprocessing/data.py:625: DataConversionWarning: Data with input dtype uint8 were all converted to float64 by StandardScaler.\n",
            "  return self.partial_fit(X, y)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/base.py:465: DataConversionWarning: Data with input dtype uint8 were all converted to float64 by StandardScaler.\n",
            "  return self.fit(X, y, **fit_params).transform(X)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/pipeline.py:331: DataConversionWarning: Data with input dtype uint8 were all converted to float64 by StandardScaler.\n",
            "  Xt = transform.transform(Xt)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/preprocessing/data.py:625: DataConversionWarning: Data with input dtype uint8 were all converted to float64 by StandardScaler.\n",
            "  return self.partial_fit(X, y)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/base.py:465: DataConversionWarning: Data with input dtype uint8 were all converted to float64 by StandardScaler.\n",
            "  return self.fit(X, y, **fit_params).transform(X)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/pipeline.py:331: DataConversionWarning: Data with input dtype uint8 were all converted to float64 by StandardScaler.\n",
            "  Xt = transform.transform(Xt)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/preprocessing/data.py:625: DataConversionWarning: Data with input dtype uint8 were all converted to float64 by StandardScaler.\n",
            "  return self.partial_fit(X, y)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/base.py:465: DataConversionWarning: Data with input dtype uint8 were all converted to float64 by StandardScaler.\n",
            "  return self.fit(X, y, **fit_params).transform(X)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/pipeline.py:331: DataConversionWarning: Data with input dtype uint8 were all converted to float64 by StandardScaler.\n",
            "  Xt = transform.transform(Xt)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "0FU8xUdnbdX3",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "TODO: That's not much better than just predicting the mean!\n",
        "\n",
        "However, the error is no worse on the test data than the cross-validated error on the training data.\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "m9Depw3LbhbG",
        "colab_type": "code",
        "outputId": "f819b456-563a-4c4f-e555-8a77e9600323",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        }
      },
      "cell_type": "code",
      "source": [
        "# MSE on test data\n",
        "X_test_le = label_encode(X_test)\n",
        "model.fit(X_label_encoded, y_train)\n",
        "y_pred = model.predict(X_test_le)\n",
        "test_mae = mean_absolute_error(y_test, y_pred)\n",
        "print('Validation MAE:', test_mae)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:55: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Validation MAE: 2.4583749152396615\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/preprocessing/data.py:625: DataConversionWarning: Data with input dtype uint8 were all converted to float64 by StandardScaler.\n",
            "  return self.partial_fit(X, y)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/base.py:465: DataConversionWarning: Data with input dtype uint8 were all converted to float64 by StandardScaler.\n",
            "  return self.fit(X, y, **fit_params).transform(X)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/pipeline.py:331: DataConversionWarning: Data with input dtype uint8 were all converted to float64 by StandardScaler.\n",
            "  Xt = transform.transform(Xt)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "g__EEWRPLuEe",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## One-hot Encoding\n",
        "\n",
        "TODO: one-hot encoding, sometimes aka \"Dummy encoding\"\n",
        "\n",
        "(note that we could also use Scikit-learn's OneHotEncoder)"
      ]
    },
    {
      "metadata": {
        "id": "fC9_QXhqLypb",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def one_hot_encode(df, cols=None, reduce_df=False, inplace=False):\n",
        "    \"\"\"One-hot encode columns.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    df : pandas DataFrame\n",
        "        Dataframe from which to one-hot encode columns\n",
        "    cols : list of str\n",
        "        Columns in df to one-hot encode\n",
        "        Default is to encode all the categorical columns in df.\n",
        "    reduce_df : bool\n",
        "        Whether to add N-1 one-hot columns for a column with N categories. \n",
        "        E.g. for a column with categories A, B, and C:\n",
        "        When reduce_df is True, A=[1, 0], B=[0, 1], and C=[0, 0]\n",
        "        When reduce_df is False, A=[1, 0, 0], B=[0, 1, 0], and C=[0, 0, 1]\n",
        "        Default = False\n",
        "    inplace : bool\n",
        "        Whether to modify df as-is (True) or create a copy (False).\n",
        "        Default = False\n",
        "        \n",
        "    Returns\n",
        "    -------\n",
        "        Nothing, modifies df in-place.\n",
        "    \"\"\"\n",
        "    \n",
        "    # Modify in-place?\n",
        "    if not inplace:\n",
        "        df = df.copy()\n",
        "\n",
        "    # Do for all \"object\" columns if not specified\n",
        "    if cols is None:\n",
        "        cols = [col for col in df if str(df[col].dtype)=='object']\n",
        "\n",
        "    # Make list if not\n",
        "    if isinstance(cols, str):\n",
        "        cols = [cols]\n",
        "\n",
        "    # Check columns are in df\n",
        "    for col in cols:\n",
        "        if col not in df:\n",
        "            raise ValueError('Column \\''+col+'\\' not in DataFrame')\n",
        "\n",
        "    # One-hot encode each column\n",
        "    for col in cols:\n",
        "        uniques = df[col].unique()\n",
        "        if len(uniques) < 2:\n",
        "            print('Warning: column '+col+' has <2 unique values, removing it')\n",
        "        elif len(uniques) == 2: #only 2 unique categories, just add binary col\n",
        "            new_col = col+'_'+str(uniques[0])\n",
        "            df[new_col] = (df[col] == uniques[0]).astype('uint8')\n",
        "        else:\n",
        "            for u_val in uniques:\n",
        "                new_col = col+'_'+str(u_val)\n",
        "                df[new_col] = (df[col] == u_val).astype('uint8')\n",
        "            if reduce_df:\n",
        "                del df[new_col]\n",
        "\n",
        "    # Delete original columns from dataframe\n",
        "    for col in cols:\n",
        "        del df[col]\n",
        "        \n",
        "    # Return the dataframe\n",
        "    return df"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "TEeLiSo2M0N6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# One-hot-encode the categorical data\n",
        "X_one_hot = one_hot_encode(X_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "AosYxSrbNBOs",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Now, instead of replacing categories with integer labels, we've created a new column for each category in each original column.  The value in a given column is 1 when the original category matches, otherwise the value is 0."
      ]
    },
    {
      "metadata": {
        "id": "Xg9TlgHDNRC-",
        "colab_type": "code",
        "outputId": "9b101082-1139-4c0d-c38e-91e677b24924",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        }
      },
      "cell_type": "code",
      "source": [
        "X_one_hot.sample(10)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>feature_0_e</th>\n",
              "      <th>feature_0_b</th>\n",
              "      <th>feature_0_d</th>\n",
              "      <th>feature_0_a</th>\n",
              "      <th>feature_0_c</th>\n",
              "      <th>feature_1_e</th>\n",
              "      <th>feature_1_d</th>\n",
              "      <th>feature_1_a</th>\n",
              "      <th>feature_1_b</th>\n",
              "      <th>feature_1_c</th>\n",
              "      <th>...</th>\n",
              "      <th>feature_8_d</th>\n",
              "      <th>feature_8_b</th>\n",
              "      <th>feature_8_a</th>\n",
              "      <th>feature_8_e</th>\n",
              "      <th>feature_8_c</th>\n",
              "      <th>feature_9_c</th>\n",
              "      <th>feature_9_a</th>\n",
              "      <th>feature_9_b</th>\n",
              "      <th>feature_9_e</th>\n",
              "      <th>feature_9_d</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2765</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4526</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4708</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>238</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2703</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1292</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2239</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3900</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4215</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1068</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>10 rows × 50 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      feature_0_e  feature_0_b  feature_0_d  feature_0_a  feature_0_c  \\\n",
              "2765            1            0            0            0            0   \n",
              "4526            0            1            0            0            0   \n",
              "4708            0            1            0            0            0   \n",
              "238             0            0            1            0            0   \n",
              "2703            0            0            1            0            0   \n",
              "1292            0            0            1            0            0   \n",
              "2239            0            0            0            0            1   \n",
              "3900            0            0            0            0            1   \n",
              "4215            0            0            1            0            0   \n",
              "1068            0            0            0            1            0   \n",
              "\n",
              "      feature_1_e  feature_1_d  feature_1_a  feature_1_b  feature_1_c  \\\n",
              "2765            0            0            0            1            0   \n",
              "4526            0            0            0            1            0   \n",
              "4708            0            0            0            0            1   \n",
              "238             0            1            0            0            0   \n",
              "2703            1            0            0            0            0   \n",
              "1292            0            0            1            0            0   \n",
              "2239            0            0            0            1            0   \n",
              "3900            0            0            0            1            0   \n",
              "4215            0            1            0            0            0   \n",
              "1068            0            0            1            0            0   \n",
              "\n",
              "         ...       feature_8_d  feature_8_b  feature_8_a  feature_8_e  \\\n",
              "2765     ...                 0            0            0            0   \n",
              "4526     ...                 0            1            0            0   \n",
              "4708     ...                 1            0            0            0   \n",
              "238      ...                 0            0            1            0   \n",
              "2703     ...                 0            0            0            1   \n",
              "1292     ...                 0            0            0            1   \n",
              "2239     ...                 0            0            0            1   \n",
              "3900     ...                 0            0            0            0   \n",
              "4215     ...                 0            0            0            0   \n",
              "1068     ...                 0            1            0            0   \n",
              "\n",
              "      feature_8_c  feature_9_c  feature_9_a  feature_9_b  feature_9_e  \\\n",
              "2765            1            0            1            0            0   \n",
              "4526            0            0            0            1            0   \n",
              "4708            0            1            0            0            0   \n",
              "238             0            0            0            0            1   \n",
              "2703            0            0            0            0            0   \n",
              "1292            0            0            0            1            0   \n",
              "2239            0            0            0            0            1   \n",
              "3900            1            0            1            0            0   \n",
              "4215            1            0            0            0            0   \n",
              "1068            0            0            0            0            1   \n",
              "\n",
              "      feature_9_d  \n",
              "2765            0  \n",
              "4526            0  \n",
              "4708            0  \n",
              "238             0  \n",
              "2703            1  \n",
              "1292            0  \n",
              "2239            0  \n",
              "3900            0  \n",
              "4215            1  \n",
              "1068            0  \n",
              "\n",
              "[10 rows x 50 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "metadata": {
        "id": "8HD4uwYjNbas",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We can fit the same model with the one-hot encoded data as we fit to the label-encoded data."
      ]
    },
    {
      "metadata": {
        "id": "zunhudmmNiTi",
        "colab_type": "code",
        "outputId": "fe9aea2d-9f52-401d-9269-af87a29b58da",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 360
        }
      },
      "cell_type": "code",
      "source": [
        "# Regression model\n",
        "model = Pipeline([\n",
        "    ('scaler', StandardScaler()),\n",
        "    ('regressor', LinearRegression())\n",
        "    #('regressor', XGBRegressor())\n",
        "])\n",
        "\n",
        "# Cross-validated MSE\n",
        "scores = cross_val_score(model, X_one_hot, y_train, \n",
        "                         cv=3, scoring=mae_scorer)\n",
        "print('Cross-validated MAE:', scores.mean(), '+/-', scores.std())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cross-validated MAE: 0.2503776209699835 +/- 0.0043136488351542355\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/preprocessing/data.py:625: DataConversionWarning: Data with input dtype uint8 were all converted to float64 by StandardScaler.\n",
            "  return self.partial_fit(X, y)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/base.py:465: DataConversionWarning: Data with input dtype uint8 were all converted to float64 by StandardScaler.\n",
            "  return self.fit(X, y, **fit_params).transform(X)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/pipeline.py:331: DataConversionWarning: Data with input dtype uint8 were all converted to float64 by StandardScaler.\n",
            "  Xt = transform.transform(Xt)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/preprocessing/data.py:625: DataConversionWarning: Data with input dtype uint8 were all converted to float64 by StandardScaler.\n",
            "  return self.partial_fit(X, y)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/base.py:465: DataConversionWarning: Data with input dtype uint8 were all converted to float64 by StandardScaler.\n",
            "  return self.fit(X, y, **fit_params).transform(X)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/pipeline.py:331: DataConversionWarning: Data with input dtype uint8 were all converted to float64 by StandardScaler.\n",
            "  Xt = transform.transform(Xt)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/preprocessing/data.py:625: DataConversionWarning: Data with input dtype uint8 were all converted to float64 by StandardScaler.\n",
            "  return self.partial_fit(X, y)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/base.py:465: DataConversionWarning: Data with input dtype uint8 were all converted to float64 by StandardScaler.\n",
            "  return self.fit(X, y, **fit_params).transform(X)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/pipeline.py:331: DataConversionWarning: Data with input dtype uint8 were all converted to float64 by StandardScaler.\n",
            "  Xt = transform.transform(Xt)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "Hu8cQWbsOHHI",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "TODO: much better!"
      ]
    },
    {
      "metadata": {
        "id": "2zvPONNFbp4v",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "TODO: compare CV performance to validation performance"
      ]
    },
    {
      "metadata": {
        "id": "pJR4JtnALzk7",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Target Encoding\n",
        "\n",
        "TODO: target encoding replaces categorical vals w/ the mean of the target for that category"
      ]
    },
    {
      "metadata": {
        "id": "eLTfbgi02ram",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class TargetEncoder(BaseEstimator, TransformerMixin):\n",
        "    \"\"\"Target encoder.\n",
        "    \n",
        "    Replaces categorical column(s) with the mean target value for\n",
        "    each category.\n",
        "\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, cols=None):\n",
        "        \"\"\"Target encoder\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        cols : list of str\n",
        "            Columns to target encode.  Default is to target-encode all \n",
        "            categorical columns in the DataFrame.\n",
        "        \"\"\"\n",
        "        if isinstance(cols, str):\n",
        "            self.cols = [cols]\n",
        "        else:\n",
        "            self.cols = cols\n",
        "        \n",
        "        \n",
        "    def fit(self, X, y):\n",
        "        \"\"\"Fit target encoder to X and y\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        X : pandas DataFrame, shape [n_samples, n_columns]\n",
        "            DataFrame containing columns to encode\n",
        "        y : pandas Series, shape = [n_samples]\n",
        "            Target values.\n",
        "            \n",
        "        Returns\n",
        "        -------\n",
        "        self : encoder\n",
        "            Returns self.\n",
        "        \"\"\"\n",
        "        \n",
        "        # Encode all categorical cols by default\n",
        "        if self.cols is None:\n",
        "            self.cols = [col for col in X if str(X[col].dtype)=='object']\n",
        "\n",
        "        # Check columns are in X\n",
        "        for col in self.cols:\n",
        "            if col not in X:\n",
        "                raise ValueError('Column \\''+col+'\\' not in X')\n",
        "\n",
        "        # Encode each element of each column\n",
        "        self.maps = dict() #dict to store map for each column\n",
        "        for col in self.cols:\n",
        "            tmap = dict()\n",
        "            uniques = X[col].unique()\n",
        "            for unique in uniques:\n",
        "                tmap[unique] = y[X[col]==unique].mean()\n",
        "            self.maps[col] = tmap\n",
        "            \n",
        "        return self\n",
        "\n",
        "        \n",
        "    def transform(self, X, y=None):\n",
        "        \"\"\"Perform the target encoding transformation.\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        X : pandas DataFrame, shape [n_samples, n_columns]\n",
        "            DataFrame containing columns to encode\n",
        "            \n",
        "        Returns\n",
        "        -------\n",
        "        pandas DataFrame\n",
        "            Input DataFrame with transformed columns\n",
        "        \"\"\"\n",
        "        Xo = X.copy()\n",
        "        for col, tmap in self.maps.items():\n",
        "            vals = np.full(X.shape[0], np.nan)\n",
        "            for val, mean_target in tmap.items():\n",
        "                vals[X[col]==val] = mean_target\n",
        "            Xo[col] = vals\n",
        "        return Xo\n",
        "            \n",
        "            \n",
        "    def fit_transform(self, X, y=None):\n",
        "        \"\"\"Fit and transform the data via target encoding.\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        X : pandas DataFrame, shape [n_samples, n_columns]\n",
        "            DataFrame containing columns to encode\n",
        "        y : pandas Series, shape = [n_samples]\n",
        "            Target values (required!).\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        pandas DataFrame\n",
        "            Input DataFrame with transformed columns\n",
        "        \"\"\"\n",
        "        return self.fit(X, y).transform(X, y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "RA5txfaXcdcA",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "TODO: cross-validated performance is great! :D\n",
        "\n",
        "TODO: but performance on test data goes way down as compared to cross-val performance!"
      ]
    },
    {
      "metadata": {
        "id": "d_CvINwf2v9f",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Cross-Fold Target Encoding\n",
        "\n",
        "TODO: problems w/ just target encoding (your performance on the test data goes way down relative to your CV score), and why it happens (b/c you're causing data leakage by allowing i-th sample's y variable to effect the value of the i-th sample's X variable which you're encoding) \n",
        "\n",
        "TODO: this can be fixed w/ cross-fold target encoding"
      ]
    },
    {
      "metadata": {
        "id": "lls4RKG-2vZ4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class TargetEncoderCV(TargetEncoder):\n",
        "    \"\"\"Cross-validated target encoder.\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, n_splits=3, shuffle=True, cols=None):\n",
        "        \"\"\"Cross-validated target encoding for categorical features.\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        n_splits : int\n",
        "            Number of cross-validation splits. Default = 3.\n",
        "        shuffle : bool\n",
        "            Whether to shuffle the data when splitting into folds.\n",
        "        cols : list of str\n",
        "            Columns to target encode.\n",
        "        \"\"\"\n",
        "        self.n_splits = n_splits\n",
        "        self.shuffle = shuffle\n",
        "        self.cols = cols\n",
        "        \n",
        "\n",
        "    def fit(self, X, y):\n",
        "        \"\"\"Fit target encoder to X and y\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        X : pandas DataFrame, shape [n_samples, n_columns]\n",
        "            DataFrame containing columns to encode\n",
        "        y : pandas Series, shape = [n_samples]\n",
        "            Target values.\n",
        "            \n",
        "        Returns\n",
        "        -------\n",
        "        self : encoder\n",
        "            Returns self.\n",
        "        \"\"\"\n",
        "        self._target_encoder = TargetEncoder(cols=self.cols)\n",
        "        self._target_encoder.fit(X, y)\n",
        "        return self\n",
        "\n",
        "    \n",
        "    def transform(self, X, y=None):\n",
        "        \"\"\"Perform the target encoding transformation.\n",
        "\n",
        "        Uses cross-validated target encoding for the training fold, and uses\n",
        "        normal target encoding for the test fold.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : pandas DataFrame, shape [n_samples, n_columns]\n",
        "            DataFrame containing columns to encode\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        pandas DataFrame\n",
        "            Input DataFrame with transformed columns\n",
        "        \"\"\"\n",
        "\n",
        "        # Use target encoding from fit() if this is test data\n",
        "        if y is None:\n",
        "            return self._target_encoder.transform(X)\n",
        "\n",
        "        # Compute means for each fold\n",
        "        self._train_ix = []\n",
        "        self._test_ix = []\n",
        "        self._fit_tes = []\n",
        "        kf = KFold(n_splits=self.n_splits, shuffle=self.shuffle)\n",
        "        for train_ix, test_ix in kf.split(X):\n",
        "            self._train_ix.append(train_ix)\n",
        "            self._test_ix.append(test_ix)\n",
        "            te = TargetEncoder(cols=self.cols)\n",
        "            if isinstance(X, pd.DataFrame):\n",
        "                self._fit_tes.append(te.fit(X.iloc[train_ix,:],\n",
        "                                            y.iloc[train_ix]))\n",
        "            elif isinstance(X, np.ndarray):\n",
        "                self._fit_tes.append(te.fit(X[train_ix,:], y[train_ix]))\n",
        "            else:\n",
        "                raise TypeError('X must be DataFrame or ndarray')\n",
        "\n",
        "        # Apply means across folds\n",
        "        Xo = X.copy()\n",
        "        for ix in range(len(self._test_ix)):\n",
        "            test_ix = self._test_ix[ix]\n",
        "            if isinstance(X, pd.DataFrame):\n",
        "                Xo.iloc[test_ix,:] = self._fit_tes[ix].transform(X.iloc[test_ix,:])\n",
        "            elif isinstance(X, np.ndarray):\n",
        "                Xo[test_ix,:] = self._fit_tes[ix].transform(X[test_ix,:])\n",
        "            else:\n",
        "                raise TypeError('X must be DataFrame or ndarray')\n",
        "        return Xo\n",
        "\n",
        "            \n",
        "    def fit_transform(self, X, y=None):\n",
        "        \"\"\"Fit and transform the data via target encoding.\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        X : pandas DataFrame, shape [n_samples, n_columns]\n",
        "            DataFrame containing columns to encode\n",
        "        y : pandas Series, shape = [n_samples]\n",
        "            Target values (required!).\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        pandas DataFrame\n",
        "            Input DataFrame with transformed columns\n",
        "        \"\"\"\n",
        "        return self.fit(X, y).transform(X, y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "RdcR2mWMRZBe",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "TODO: cross-val performance (compare to cross-val performance of one-hot, and cross-val performance of naive target encoder)\n",
        "\n",
        "TODO: performance on validation data (compare to val performance of one-hot and naive target encoder)"
      ]
    },
    {
      "metadata": {
        "id": "hNryKyVCM0cr",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Dependence on Number of Categories\n",
        "\n",
        "TODO: not using cross-fold TE is worse when there are more categories.\n",
        "\n",
        "TODO: but if you have a very small number of categories, you may be better off using one-hot (compare mae's as a fn of #categories/#samples)"
      ]
    },
    {
      "metadata": {
        "id": "nACpTFNGM-57",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "pXAZQCjmNiYz",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Effect of Interactions\n",
        "\n",
        "TODO: target encoding doesn't capture interactions very well, which one-hot does (in theory?  simulate data w/ important interaction effects and see if it does better than target encoder)"
      ]
    },
    {
      "metadata": {
        "id": "a-pdSDv8cvZH",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# TODO: add some sort of interactions param to make_categorical_regression func?"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}